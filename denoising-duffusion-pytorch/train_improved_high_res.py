import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time

# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False

def get_improved_high_res_dataloader(
    dataset_name='stl10', 
    batch_size=16, 
    num_workers=None,
    use_upsampling=False,
    target_size=None,
    progressive_training=False,
    current_size=None
):
    """è·å–æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒæ¸è¿›å¼è®­ç»ƒ"""
    
    if num_workers is None:
        num_workers = min(6, os.cpu_count() or 4)  # ç¨å¾®ä¿å®ˆä¸€äº›
    
    # æ ¹æ®æ•°æ®é›†ç¡®å®šåŸç”Ÿåˆ†è¾¨ç‡
    if dataset_name.lower() == 'cifar10':
        native_size = 32
    elif dataset_name.lower() == 'stl10':
        native_size = 96
    elif dataset_name.lower() == 'celeba':
        native_size = 64
    else:
        native_size = 64
    
    # æ¸è¿›å¼è®­ç»ƒï¼šä»å°å°ºå¯¸å¼€å§‹
    if progressive_training and current_size is not None:
        image_size = current_size
        print(f"ğŸ”„ æ¸è¿›å¼è®­ç»ƒ: å½“å‰åˆ†è¾¨ç‡ {image_size}x{image_size}")
    else:
        image_size = target_size if target_size else native_size
    
    print(f"ğŸ“Š æ”¹è¿›çš„æ•°æ®é›†é…ç½®:")
    print(f"   - æ•°æ®é›†: {dataset_name.upper()}")
    print(f"   - åŸç”Ÿåˆ†è¾¨ç‡: {native_size}x{native_size}")
    print(f"   - è®­ç»ƒåˆ†è¾¨ç‡: {image_size}x{image_size}")
    print(f"   - æ¸è¿›å¼è®­ç»ƒ: {'æ˜¯' if progressive_training else 'å¦'}")
    
    # æ„å»ºå˜æ¢ç®¡é“
    transforms_list = []
    
    # æ™ºèƒ½å°ºå¯¸è°ƒæ•´ç­–ç•¥
    if image_size != native_size:
        if image_size < native_size:
            # ä¸‹é‡‡æ ·ï¼šä¿æŒè´¨é‡
            transforms_list.extend([
                transforms.Resize(int(image_size * 1.1)),  # ç¨å¾®å¤§ä¸€ç‚¹å†crop
                transforms.CenterCrop(image_size)
            ])
        else:
            # ä¸Šé‡‡æ ·ï¼šæé«˜è´¨é‡
            if use_upsampling:
                transforms_list.extend([
                    transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),
                    transforms.CenterCrop(image_size)
                ])
                print("ğŸ”„ ä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼ä¸Šé‡‡æ ·")
            else:
                raise ValueError(f"ç›®æ ‡å°ºå¯¸{image_size}å¤§äºåŸç”Ÿå°ºå¯¸{native_size}ï¼Œè¯·è®¾ç½®use_upsampling=True")
    
    # å¢å¼ºçš„æ•°æ®å¢å¼ºç­–ç•¥
    augment_transforms = []  # type: ignore
    augment_transforms.append(transforms.RandomHorizontalFlip(p=0.5))
    
    # æ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´å¢å¼ºå¼ºåº¦
    if image_size >= 64:
        augment_transforms.append(transforms.RandomRotation(degrees=3))  # è½»å¾®æ—‹è½¬
        # è½»å¾®é¢œè‰²å¢å¼ºï¼Œæœ‰åŠ©äºæ¨¡å‹æ³›åŒ–
        augment_transforms.append(transforms.ColorJitter(
            brightness=0.05,
            contrast=0.05,
            saturation=0.05,
            hue=0.025
        ))
    
    # æœ€ç»ˆå˜æ¢
    final_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]
    
    # ç»„åˆæ‰€æœ‰å˜æ¢
    transform = transforms.Compose(transforms_list + augment_transforms + final_transforms)
    
    # åˆ›å»ºæ•°æ®é›†
    if dataset_name.lower() == 'cifar10':
        dataset = torchvision.datasets.CIFAR10(
            root='./data',
            train=True,
            download=True,
            transform=transform
        )
    elif dataset_name.lower() == 'stl10':
        dataset = torchvision.datasets.STL10(
            root='./data',
            split='train',
            download=True,
            transform=transform
        )
    elif dataset_name.lower() == 'celeba':
        try:
            celeba_transform = transforms.Compose([
                transforms.CenterCrop(178),
                transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),
                *augment_transforms,
                *final_transforms
            ])
            
            dataset = torchvision.datasets.CelebA(
                root='./data',
                split='train',
                download=False,
                transform=celeba_transform
            )
        except Exception as e:
            print(f"âŒ CelebAåŠ è½½å¤±è´¥: {e}")
            raise
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=2,
        drop_last=True
    )
    
    return dataloader, dataset, image_size

def get_improved_model_config(image_size, quality_mode='balanced'):
    """è·å–æ”¹è¿›çš„æ¨¡å‹é…ç½®"""
    
    if quality_mode == 'fast':
        # å¿«é€Ÿè®­ç»ƒæ¨¡å¼
        if image_size <= 32:
            return {'dim': 64, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 64:
            return {'dim': 96, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 96:
            return {'dim': 128, 'dim_mults': (1, 2, 4, 8)}
        else:
            return {'dim': 160, 'dim_mults': (1, 2, 4, 8)}
    
    elif quality_mode == 'balanced':
        # å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰
        if image_size <= 32:
            return {'dim': 128, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 64:
            return {'dim': 160, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 96:
            return {'dim': 192, 'dim_mults': (1, 2, 4, 8)}
        else:
            return {'dim': 256, 'dim_mults': (1, 1, 2, 2, 4, 4)}
    
    else:  # quality_mode == 'high'
        # é«˜è´¨é‡æ¨¡å¼
        if image_size <= 32:
            return {'dim': 192, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 64:
            return {'dim': 256, 'dim_mults': (1, 2, 4, 8)}
        elif image_size <= 96:
            return {'dim': 320, 'dim_mults': (1, 2, 4, 8)}
        else:
            return {'dim': 384, 'dim_mults': (1, 1, 2, 2, 4, 4)}

def train_improved_high_res_ddpm(
    dataset_name='stl10',
    target_size=None,
    use_upsampling=False,
    epochs=150,  # å¤§å¹…å¢åŠ è®­ç»ƒè½®æ•°
    batch_size=None,
    learning_rate=2e-4,  # ç¨å¾®æé«˜å­¦ä¹ ç‡
    save_interval=20,
    use_amp=True,
    compile_model=True,
    quality_mode='balanced',  # 'fast', 'balanced', 'high'
    progressive_training=False,  # æ¸è¿›å¼è®­ç»ƒ
    warmup_epochs=20,  # å­¦ä¹ ç‡çƒ­èº«
    early_sample_freq=3  # æ—©æœŸæ›´é¢‘ç¹é‡‡æ ·
):
    """æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡DDPMè®­ç»ƒï¼Œè§£å†³æ•ˆæœä¸ç†æƒ³é—®é¢˜"""
    
    print("ğŸ¯ æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡DDPMè®­ç»ƒ")
    print("é‡ç‚¹è§£å†³ç”Ÿæˆæ•ˆæœä¸ç†æƒ³çš„é—®é¢˜")
    print("=" * 60)
    
    # è·å–GPUä¿¡æ¯å¹¶æ™ºèƒ½è°ƒæ•´é…ç½®
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        if '3060' in gpu_name:
            gpu_memory = 12
            if quality_mode == 'high':
                print("âš ï¸ RTX 3060æ£€æµ‹åˆ°é«˜è´¨é‡æ¨¡å¼ï¼Œå»ºè®®ä½¿ç”¨balancedæ¨¡å¼")
                quality_mode = 'balanced'
        elif '3090' in gpu_name or '4090' in gpu_name:
            gpu_memory = 24
        elif '3080' in gpu_name:
            gpu_memory = 10
        else:
            gpu_memory = 8
            if quality_mode == 'high':
                quality_mode = 'balanced'
        print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({gpu_memory}GB)")
    else:
        gpu_memory = 0
        quality_mode = 'fast'
        print("ğŸ–¥ï¸  è®¾å¤‡: CPU (å¼ºåˆ¶å¿«é€Ÿæ¨¡å¼)")
    
    # è·å–æ•°æ®åŠ è½½å™¨å¹¶ç¡®å®šå›¾åƒå°ºå¯¸
    dataloader, dataset, image_size = get_improved_high_res_dataloader(
        dataset_name=dataset_name,
        target_size=target_size,
        use_upsampling=use_upsampling,
        batch_size=batch_size or 16
    )
    
    # æ™ºèƒ½æ‰¹æ¬¡å¤§å°è°ƒæ•´
    if batch_size is None:
        memory_factor = gpu_memory / 12  # ä»¥12GBä¸ºåŸºå‡†
        if image_size <= 32:
            batch_size = max(4, int(32 * memory_factor))
        elif image_size <= 64:
            batch_size = max(4, int(16 * memory_factor))
        elif image_size <= 96:
            batch_size = max(2, int(8 * memory_factor))
        else:
            batch_size = max(2, int(4 * memory_factor))
    
    # é‡æ–°åˆ›å»ºdataloader withæ­£ç¡®çš„batch_size
    dataloader, dataset, image_size = get_improved_high_res_dataloader(
        dataset_name=dataset_name,
        target_size=target_size,
        use_upsampling=use_upsampling,
        batch_size=batch_size
    )
    
    # è·å–æ”¹è¿›çš„æ¨¡å‹é…ç½®
    model_config = get_improved_model_config(image_size, quality_mode)
    
    print(f"ğŸ—ï¸  æ”¹è¿›çš„æ¨¡å‹é…ç½®:")
    print(f"   - è´¨é‡æ¨¡å¼: {quality_mode}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ¨¡å‹ç»´åº¦: {model_config['dim']}")
    print(f"   - å±‚çº§å€æ•°: {model_config['dim_mults']}")
    print(f"   - è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   - çƒ­èº«è½®æ•°: {warmup_epochs}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=image_size,
        channels=3,
        timesteps=1000,
        **model_config
    )
    
    total_params = sum(p.numel() for p in model.unet.parameters())
    print(f"   - æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # æ¨¡å‹ç¼–è¯‘
    if compile_model and hasattr(torch, 'compile'):
        try:
            compiled_unet = torch.compile(model.unet, mode='reduce-overhead')
            model.unet = compiled_unet  # type: ignore
            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
    
    # æ”¹è¿›çš„ä¼˜åŒ–å™¨é…ç½®
    optimizer = optim.AdamW(
        model.unet.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    warmup_scheduler = optim.lr_scheduler.LinearLR(
        optimizer, 
        start_factor=0.1, 
        end_factor=1.0, 
        total_iters=warmup_epochs
    )
    
    main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=epochs - warmup_epochs, 
        eta_min=learning_rate * 0.01
    )
    
    # æ··åˆç²¾åº¦
    scaler = GradScaler() if use_amp and AMP_AVAILABLE else None
    if scaler:
        print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    exp_name = f"{dataset_name}_improved_{image_size}x{image_size}_{quality_mode}"
    os.makedirs(f'checkpoints/{exp_name}', exist_ok=True)
    os.makedirs(f'samples/{exp_name}', exist_ok=True)
    os.makedirs(f'training_progress/{exp_name}', exist_ok=True)
    
    print(f"ğŸ“ å®éªŒåç§°: {exp_name}")
    print("=" * 70)
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    best_loss = float('inf')
    training_start_time = time.time()
    
    for epoch in range(epochs):
        model.unet.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if epoch < warmup_epochs:
            warmup_scheduler.step()
        else:
            main_scheduler.step()
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            optimizer.zero_grad()
            
            if use_amp and scaler is not None:
                with autocast():
                    loss = model.train_step(data)
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.unet.parameters(), max_norm=1.0)
                
                scaler.step(optimizer)
                scaler.update()
            else:
                loss = model.train_step(data)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.unet.parameters(), max_norm=1.0)
                
                optimizer.step()
            
            epoch_losses.append(loss.item())
            
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{current_lr:.2e}',
                'mode': quality_mode
            })
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'ğŸ“Š Epoch {epoch+1}/{epochs}:')
        print(f'   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’')
        print(f'   ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'   ğŸ“ˆ å­¦ä¹ ç‡: {current_lr:.2e}')
        
        # æ”¹è¿›çš„è´¨é‡è¯„ä¼°
        if avg_loss > 0.8:
            quality = "ğŸ”´ åˆå§‹å™ªå£°é˜¶æ®µ"
        elif avg_loss > 0.5:
            quality = "ğŸŸ¡ åŸºç¡€ç»“æ„å½¢æˆ"
        elif avg_loss > 0.3:
            quality = "ğŸŸ¢ å½¢çŠ¶ç»†èŠ‚æ˜¾ç°"
        elif avg_loss > 0.15:
            quality = "ğŸ”µ å¯è¾¨è¯†ç‰©ä½“"
        elif avg_loss > 0.08:
            quality = "ğŸŸ£ é«˜è´¨é‡ç»†èŠ‚"
        else:
            quality = "â­ è¶…é«˜è´¨é‡"
        
        print(f'   ğŸ¯ è´¨é‡è¯„ä¼°: {quality}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = f'checkpoints/{exp_name}/ddpm_best.pt'
            model.save_model(best_path)
            print(f'   ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_loss:.4f})')
        
        # æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥ï¼šæ—©æœŸæ›´é¢‘ç¹ï¼ŒåæœŸless frequent
        sample_freq = early_sample_freq if epoch < epochs // 3 else 5
        if (epoch + 1) % sample_freq == 0:
            model.unet.eval()
            with torch.no_grad():
                # å¤šæ ·åŒ–é‡‡æ ·
                sample_count = min(9, batch_size)
                quick_samples = model.sample(batch_size=sample_count)
                save_improved_samples(
                    quick_samples,
                    f'training_progress/{exp_name}/quick_epoch_{epoch+1}.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    image_size=image_size,
                    quality=quality,
                    dataset_name=dataset_name
                )
            print(f'   ğŸ“¸ å¿«é€Ÿé‡‡æ ·å·²ä¿å­˜')
        
        # å®šæœŸä¿å­˜å’Œè¯¦ç»†é‡‡æ ·
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoints/{exp_name}/ddpm_epoch_{epoch+1}.pt'
            model.save_model(checkpoint_path)
            
            model.unet.eval()
            with torch.no_grad():
                # ç”Ÿæˆæ›´å¤šæ ·æœ¬ç”¨äºè´¨é‡è¯„ä¼°
                samples = model.sample(batch_size=16)
                save_improved_samples(
                    samples,
                    f'samples/{exp_name}/epoch_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    image_size=image_size,
                    quality=quality,
                    dataset_name=dataset_name
                )
                
                # ç”Ÿæˆé¢å¤–çš„å¯¹æ¯”æ ·æœ¬
                if epoch > epochs // 2:  # åæœŸè¿›è¡Œqualityå¯¹æ¯”
                    try:
                        # ç”Ÿæˆæ›´å¤šæ ·æœ¬ç”¨äºå¯¹æ¯”
                        extra_samples = model.sample(batch_size=4)
                        save_improved_samples(
                            extra_samples,
                            f'samples/{exp_name}/epoch_{epoch+1}_extra.png',
                            epoch=epoch+1,
                            loss=avg_loss,
                            image_size=image_size,
                            quality=quality + " (é¢å¤–æ ·æœ¬)",
                            dataset_name=dataset_name
                        )
                    except Exception as e:
                        print(f"   âš ï¸ é¢å¤–é‡‡æ ·å¤±è´¥: {e}")
            
            plot_improved_losses(losses, f'samples/{exp_name}/loss_curve_epoch_{epoch+1}.png', exp_name)
            print(f'   ğŸ’¾ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜')
        
        # æŸå¤±è¶‹åŠ¿åˆ†æ
        if len(losses) >= 10:
            recent_trend = np.mean(losses[-5:]) - np.mean(losses[-10:-5])
            if recent_trend > 0.01:
                print(f'   âš ï¸ æŸå¤±ä¸Šå‡è¶‹åŠ¿æ£€æµ‹åˆ° (+{recent_trend:.4f})')
            elif recent_trend < -0.01:
                print(f'   âœ… æŸå¤±ä¸‹é™è‰¯å¥½ ({recent_trend:.4f})')
        
        print("-" * 60)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - training_start_time
    final_path = f'checkpoints/{exp_name}/ddpm_final.pt'
    model.save_model(final_path)
    
    # æœ€ç»ˆè´¨é‡è¯„ä¼°
    model.unet.eval()
    with torch.no_grad():
        print("ğŸ¯ ç”Ÿæˆæœ€ç»ˆè´¨é‡è¯„ä¼°æ ·æœ¬...")
        final_samples = model.sample(batch_size=25)  # 5x5ç½‘æ ¼
        save_improved_samples(
            final_samples,
            f'samples/{exp_name}/final_quality_assessment.png',
            epoch=epochs,
            loss=best_loss,
            image_size=image_size,
            quality="æœ€ç»ˆè´¨é‡è¯„ä¼°",
            dataset_name=dataset_name
        )
    
    print("ğŸ‰ æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸ“‰ æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    print(f"ğŸ–¼ï¸  æœ€ç»ˆæ ·æœ¬: samples/{exp_name}/final_quality_assessment.png")
    
    return model, losses

def save_improved_samples(samples, path, epoch=None, loss=None, image_size=64, quality="", dataset_name=""):
    """ä¿å­˜æ”¹è¿›çš„æ ·æœ¬ï¼Œå¢å¼ºå¯è§†åŒ–"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        # åŠ¨æ€è°ƒæ•´ç½‘æ ¼
        if len(samples) <= 4:
            nrow = 2
        elif len(samples) <= 9:
            nrow = 3
        elif len(samples) <= 16:
            nrow = 4
        else:
            nrow = 5
        
        grid = torchvision.utils.make_grid(samples, nrow=nrow, padding=3, pad_value=1.0)
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        # åŠ¨æ€è°ƒæ•´å›¾åƒå¤§å°
        fig_size = max(10, min(16, image_size // 8))
        plt.figure(figsize=(fig_size, fig_size))
        plt.imshow(grid_np)
        plt.axis('off')
        
        # æ”¹è¿›çš„æ ‡é¢˜
        title_parts = []
        if dataset_name:
            title_parts.append(f'{dataset_name.upper()}')
        if epoch is not None:
            title_parts.append(f'Epoch {epoch}')
        if loss is not None:
            title_parts.append(f'Loss: {loss:.4f}')
        title_parts.append(f'({image_size}x{image_size})')
        if quality:
            title_parts.append(f'\n{quality}')
        
        title = ' - '.join(title_parts[:4])
        if len(title_parts) > 4:
            title += title_parts[4]
        
        plt.title(title, fontsize=12, fontweight='bold', pad=20)
        
        # é«˜è´¨é‡ä¿å­˜
        dpi = 300 if image_size >= 96 else 200
        plt.savefig(path, bbox_inches='tight', dpi=dpi, facecolor='white')
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_improved_losses(losses, path, exp_name):
    """ç»˜åˆ¶æ”¹è¿›çš„è®­ç»ƒæŸå¤±æ›²çº¿"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(14, 8))
        
        # åŸå§‹æŸå¤±æ›²çº¿
        plt.subplot(2, 1, 1)
        plt.plot(losses, linewidth=2, color='blue', alpha=0.7, label='è®­ç»ƒæŸå¤±')
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡
        if len(losses) > 10:
            window = min(10, len(losses) // 5)
            moving_avg = np.convolve(losses, np.ones(window)/window, mode='valid')
            plt.plot(range(window-1, len(losses)), moving_avg, 
                    linewidth=3, color='red', label=f'{window}è½®ç§»åŠ¨å¹³å‡')
        
        plt.title(f'Training Loss - {exp_name}', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # æ·»åŠ è´¨é‡é˜¶æ®µæ ‡è®°
        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='åˆå§‹å™ªå£°')
        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='åŸºç¡€ç»“æ„')
        plt.axhline(y=0.3, color='yellow', linestyle='--', alpha=0.5, label='å½¢çŠ¶ç»†èŠ‚')
        plt.axhline(y=0.15, color='green', linestyle='--', alpha=0.5, label='å¯è¾¨è¯†ç‰©ä½“')
        plt.axhline(y=0.08, color='blue', linestyle='--', alpha=0.5, label='é«˜è´¨é‡')
        
        # æŸå¤±æ”¹å–„ç‡
        plt.subplot(2, 1, 2)
        if len(losses) > 1:
            improvements = np.diff(losses)
            plt.plot(improvements, linewidth=2, color='green', alpha=0.7)
            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            plt.title('Loss Improvement Rate', fontsize=14)
            plt.xlabel('Epoch', fontsize=12)
            plt.ylabel('Loss Change', fontsize=12)
            plt.grid(True, alpha=0.3)
        
        # æ ‡è®°æœ€ä½³ç‚¹
        if losses:
            min_loss_idx = np.argmin(losses)
            min_loss = losses[min_loss_idx]
            plt.subplot(2, 1, 1)
            plt.scatter([min_loss_idx], [min_loss], color='red', s=100, zorder=5)
            plt.annotate(f'Best: {min_loss:.4f}', 
                        xy=(float(min_loss_idx), float(min_loss)),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(path, bbox_inches='tight', dpi=150, facecolor='white')
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    print("ğŸ¯ æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡DDPMè®­ç»ƒ")
    print("ä¸“é—¨è§£å†³ç”Ÿæˆæ•ˆæœä¸ç†æƒ³çš„é—®é¢˜")
    print()
    print("æ¨èé…ç½®:")
    print("  1. STL-10å¹³è¡¡æ¨¡å¼: python train_improved_high_res.py stl10 --quality balanced")
    print("  2. STL-10é«˜è´¨é‡: python train_improved_high_res.py stl10 --quality high")
    print("  3. CelebAäººè„¸: python train_improved_high_res.py celeba --quality balanced")
    print()
    
    # è§£æå‚æ•°
    dataset_name = sys.argv[1] if len(sys.argv) > 1 else 'stl10'
    
    # è§£æè´¨é‡æ¨¡å¼
    quality_mode = 'balanced'
    if '--quality' in sys.argv:
        quality_idx = sys.argv.index('--quality')
        if quality_idx + 1 < len(sys.argv):
            quality_mode = sys.argv[quality_idx + 1]
    
    # è§£æè®­ç»ƒè½®æ•°
    epochs = 150
    if '--epochs' in sys.argv:
        epochs_idx = sys.argv.index('--epochs')
        if epochs_idx + 1 < len(sys.argv):
            epochs = int(sys.argv[epochs_idx + 1])
    
    print(f"é€‰æ‹©çš„æ•°æ®é›†: {dataset_name}")
    print(f"è´¨é‡æ¨¡å¼: {quality_mode}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print("=" * 60)
    
    try:
        model, losses = train_improved_high_res_ddpm(
            dataset_name=dataset_name,
            epochs=epochs,
            quality_mode=quality_mode,
            use_amp='--no-amp' not in sys.argv,
            compile_model='--no-compile' not in sys.argv,
            progressive_training='--progressive' in sys.argv
        )
        
        print("ğŸ‰ æ”¹è¿›çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥æœ€ç»ˆè´¨é‡è¯„ä¼°å›¾åƒä»¥éªŒè¯æ•ˆæœ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()