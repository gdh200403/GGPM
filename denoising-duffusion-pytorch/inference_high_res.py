import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time
from PIL import Image

def load_high_res_model(model_path, device='auto'):
    """åŠ è½½é«˜åˆ†è¾¨ç‡æ¨¡å‹"""
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # å°è¯•ä»è·¯å¾„æ¨æ–­é…ç½®
    filename = os.path.basename(model_path)
    if 'cifar10_64x64' in filename:
        image_size = 64
        config = {'dim': 128, 'dim_mults': (1, 2, 4, 8)}
    elif 'cifar10_128x128' in filename:
        image_size = 128
        config = {'dim': 192, 'dim_mults': (1, 1, 2, 2, 4, 4)}
    elif 'cifar10_256x256' in filename:
        image_size = 256
        config = {'dim': 256, 'dim_mults': (1, 1, 2, 2, 4, 4, 8)}
    else:
        # é»˜è®¤é…ç½®
        image_size = 64
        config = {'dim': 128, 'dim_mults': (1, 2, 4, 8)}
        print("âš ï¸ æ— æ³•ä»æ–‡ä»¶åæ¨æ–­é…ç½®ï¼Œä½¿ç”¨é»˜è®¤64x64é…ç½®")
    
    model = DDPMModel(
        image_size=image_size,
        channels=3,
        timesteps=1000,
        device=device,
        **config
    )
    
    model.load_model(model_path)
    model.unet.eval()
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.unet.parameters())
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   - å›¾åƒå°ºå¯¸: {image_size}x{image_size}")
    print(f"   - å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   - æ¨¡å‹ç»´åº¦: {config['dim']}")
    
    return model, image_size

def generate_high_res_samples(
    model, 
    num_samples=16, 
    save_path=None, 
    sampling_steps=None,
    guidance_scale=1.0,
    show_progress=True
):
    """ç”Ÿæˆé«˜åˆ†è¾¨ç‡æ ·æœ¬"""
    
    print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆ {num_samples} ä¸ªé«˜åˆ†è¾¨ç‡æ ·æœ¬")
    
    device = model.device
    
    # æ ¹æ®æ¨¡å‹è‡ªåŠ¨è°ƒæ•´é‡‡æ ·æ­¥æ•°
    if sampling_steps is None:
        if model.image_size <= 64:
            sampling_steps = 250
        elif model.image_size <= 128:
            sampling_steps = 500
        else:
            sampling_steps = 1000
    
    print(f"âš™ï¸  é‡‡æ ·æ­¥æ•°: {sampling_steps}")
    print(f"ğŸ¯ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
    
    start_time = time.time()
    
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡å‹çš„diffusionå¯¹è±¡è¿›è¡Œé‡‡æ ·
        samples = model.diffusion.sample(batch_size=num_samples)
    
    generation_time = time.time() - start_time
    print(f"â±ï¸  ç”Ÿæˆç”¨æ—¶: {generation_time:.2f}ç§’")
    print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {num_samples/generation_time:.1f} æ ·æœ¬/ç§’")
    
    # ä¿å­˜æ ·æœ¬
    if save_path:
        save_high_quality_samples(samples, save_path, model.image_size)
        print(f"ğŸ’¾ æ ·æœ¬å·²ä¿å­˜åˆ°: {save_path}")
    
    return samples

def save_high_quality_samples(samples, path, image_size):
    """ä¿å­˜é«˜è´¨é‡æ ·æœ¬"""
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
    
    # è½¬æ¢åˆ°[0,1]èŒƒå›´
    samples = (samples + 1) / 2
    samples = torch.clamp(samples, 0, 1)
    
    # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´ç½‘æ ¼
    n_samples = len(samples)
    if n_samples <= 4:
        nrow = 2
    elif n_samples <= 9:
        nrow = 3
    elif n_samples <= 16:
        nrow = 4
    elif n_samples <= 25:
        nrow = 5
    else:
        nrow = int(np.sqrt(n_samples))
    
    # åˆ›å»ºç½‘æ ¼
    grid = torchvision.utils.make_grid(samples, nrow=nrow, padding=4)
    grid_np = grid.permute(1, 2, 0).cpu().numpy()
    
    # æ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´æ˜¾ç¤ºå¤§å°
    fig_size = min(20, max(10, image_size // 8))
    plt.figure(figsize=(fig_size, fig_size))
    plt.imshow(grid_np)
    plt.axis('off')
    plt.title(f'Generated Samples ({image_size}x{image_size})', 
              fontsize=16, fontweight='bold', pad=20)
    
    # é«˜è´¨é‡ä¿å­˜
    dpi = 300 if image_size >= 128 else 200
    plt.savefig(path, bbox_inches='tight', dpi=dpi, facecolor='white')
    plt.close()
    
    # åŒæ—¶ä¿å­˜å•ç‹¬çš„å›¾ç‰‡
    base_name = os.path.splitext(path)[0]
    individual_dir = f"{base_name}_individual"
    os.makedirs(individual_dir, exist_ok=True)
    
    for i, sample in enumerate(samples):
        sample_np = sample.permute(1, 2, 0).cpu().numpy()
        sample_np = (sample_np * 255).astype(np.uint8)
        img = Image.fromarray(sample_np)
        img.save(f"{individual_dir}/sample_{i+1:03d}.png")

def interpolate_high_res(model, num_steps=10, save_path=None):
    """é«˜åˆ†è¾¨ç‡æ’å€¼"""
    print(f"ğŸ”„ ç”Ÿæˆé«˜åˆ†è¾¨ç‡æ’å€¼åºåˆ— ({num_steps} æ­¥)")
    
    device = model.device
    
    with torch.no_grad():
        # ç”Ÿæˆä¸¤ä¸ªéšæœºæ ·æœ¬
        sample1 = model.sample(batch_size=1)
        sample2 = model.sample(batch_size=1)
        
        # ä½¿ç”¨æ¨¡å‹çš„æ’å€¼æ–¹æ³•
        interpolated = model.interpolate(sample1, sample2, num_steps=num_steps)
    
    if save_path:
        save_interpolation_grid(interpolated, save_path, model.image_size)
        print(f"ğŸ’¾ æ’å€¼åºåˆ—å·²ä¿å­˜åˆ°: {save_path}")
    
    return interpolated

def save_interpolation_grid(samples, path, image_size):
    """ä¿å­˜æ’å€¼ç½‘æ ¼"""
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
    
    samples = (samples + 1) / 2
    samples = torch.clamp(samples, 0, 1)
    
    # å•è¡Œç½‘æ ¼æ˜¾ç¤ºæ’å€¼è¿‡ç¨‹
    grid = torchvision.utils.make_grid(samples, nrow=len(samples), padding=2)
    grid_np = grid.permute(1, 2, 0).cpu().numpy()
    
    plt.figure(figsize=(len(samples) * 2, 2))
    plt.imshow(grid_np)
    plt.axis('off')
    plt.title(f'Interpolation Sequence ({image_size}x{image_size})', 
              fontsize=14, fontweight='bold')
    
    plt.savefig(path, bbox_inches='tight', dpi=200, facecolor='white')
    plt.close()

def compare_resolutions(model_paths, num_samples=4):
    """æ¯”è¾ƒä¸åŒåˆ†è¾¨ç‡çš„ç”Ÿæˆæ•ˆæœ"""
    print("ğŸ“Š æ¯”è¾ƒä¸åŒåˆ†è¾¨ç‡çš„ç”Ÿæˆæ•ˆæœ")
    
    results = {}
    
    for model_path in model_paths:
        if os.path.exists(model_path):
            try:
                model, image_size = load_high_res_model(model_path)
                samples = generate_high_res_samples(
                    model, 
                    num_samples=num_samples, 
                    show_progress=False
                )
                results[f"{image_size}x{image_size}"] = samples
                print(f"âœ… {image_size}x{image_size} æ¨¡å‹æµ‹è¯•å®Œæˆ")
            except Exception as e:
                print(f"âŒ {model_path} åŠ è½½å¤±è´¥: {e}")
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    if results:
        # ä¿å­˜æ¯”è¾ƒå›¾
        save_comparison_grid(results, "resolution_comparison.png")
        print("ğŸ’¾ åˆ†è¾¨ç‡æ¯”è¾ƒå›¾å·²ä¿å­˜")
    
    return results

def save_comparison_grid(results, path):
    """ä¿å­˜åˆ†è¾¨ç‡æ¯”è¾ƒç½‘æ ¼"""
    if not results:
        return
    
    fig, axes = plt.subplots(len(results), 4, figsize=(16, 4 * len(results)))
    if len(results) == 1:
        axes = axes.reshape(1, -1)
    
    for row, (resolution, samples) in enumerate(results.items()):
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        for col in range(min(4, len(samples))):
            sample_np = samples[col].permute(1, 2, 0).cpu().numpy()
            axes[row, col].imshow(sample_np)
            axes[row, col].axis('off')
            if col == 0:
                axes[row, col].set_ylabel(resolution, fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(path, bbox_inches='tight', dpi=200, facecolor='white')
    plt.close()

def main():
    """ä¸»å‡½æ•° - é«˜åˆ†è¾¨ç‡æ¨ç†ç¤ºä¾‹"""
    import argparse
    
    parser = argparse.ArgumentParser(description='é«˜åˆ†è¾¨ç‡DDPMæ¨ç†')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--samples', type=int, default=16, help='ç”Ÿæˆæ ·æœ¬æ•°é‡')
    parser.add_argument('--steps', type=int, default=None, help='é‡‡æ ·æ­¥æ•°')
    parser.add_argument('--output', type=str, default='high_res_samples.png', help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--interpolate', action='store_true', help='ç”Ÿæˆæ’å€¼åºåˆ—')
    parser.add_argument('--compare', nargs='+', help='æ¯”è¾ƒå¤šä¸ªæ¨¡å‹')
    
    args = parser.parse_args()
    
    if args.compare:
        # æ¯”è¾ƒæ¨¡å¼
        compare_resolutions(args.compare)
    else:
        # å•æ¨¡å‹ç”Ÿæˆ
        model, image_size = load_high_res_model(args.model)
        
        # ç”Ÿæˆæ ·æœ¬
        samples = generate_high_res_samples(
            model, 
            num_samples=args.samples,
            save_path=args.output,
            sampling_steps=args.steps
        )
        
        # æ’å€¼ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if args.interpolate:
            interpolate_path = f"interpolation_{image_size}x{image_size}.png"
            interpolate_high_res(model, save_path=interpolate_path)

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œä½¿ç”¨äº¤äº’æ¨¡å¼
    print("ğŸ¨ é«˜åˆ†è¾¨ç‡DDPMæ¨ç†å·¥å…·")
    print("=" * 50)
    
    # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹
    checkpoint_dirs = [d for d in os.listdir('.') if d.startswith('checkpoints') and os.path.isdir(d)]
    
    if not checkpoint_dirs:
        print("âŒ æœªæ‰¾åˆ°checkpointsç›®å½•")
        exit(1)
    
    # æ‰«æé«˜åˆ†è¾¨ç‡æ¨¡å‹
    high_res_models = []
    for checkpoint_dir in checkpoint_dirs:
        for root, dirs, files in os.walk(checkpoint_dir):
            for file in files:
                if file.endswith('.pt') and any(res in root for res in ['64x64', '128x128', '256x256']):
                    high_res_models.append(os.path.join(root, file))
    
    if not high_res_models:
        print("âŒ æœªæ‰¾åˆ°é«˜åˆ†è¾¨ç‡æ¨¡å‹æ–‡ä»¶")
        print("è¯·å…ˆè®­ç»ƒé«˜åˆ†è¾¨ç‡æ¨¡å‹ï¼špython train_high_res.py --dataset=cifar10 --size=64")
        exit(1)
    
    print("ğŸ“‚ å‘ç°çš„é«˜åˆ†è¾¨ç‡æ¨¡å‹:")
    for i, model_path in enumerate(high_res_models):
        print(f"   {i+1}. {model_path}")
    
    try:
        choice = int(input("\né€‰æ‹©æ¨¡å‹ (è¾“å…¥ç¼–å·): ")) - 1
        if 0 <= choice < len(high_res_models):
            model_path = high_res_models[choice]
            
            model, image_size = load_high_res_model(model_path)
            
            print("\nğŸ¯ ç”Ÿæˆé€‰é¡¹:")
            print("1. ç”Ÿæˆæ ·æœ¬")
            print("2. ç”Ÿæˆæ’å€¼åºåˆ—") 
            print("3. æ‰¹é‡ç”Ÿæˆ")
            
            option = input("é€‰æ‹©æ“ä½œ (1-3): ")
            
            if option == "1":
                num_samples = int(input("æ ·æœ¬æ•°é‡ (é»˜è®¤16): ") or "16")
                output_path = f"generated_samples_{image_size}x{image_size}.png"
                generate_high_res_samples(model, num_samples=num_samples, save_path=output_path)
                
            elif option == "2":
                interpolate_high_res(model, save_path=f"interpolation_{image_size}x{image_size}.png")
                
            elif option == "3":
                batch_size = int(input("æ¯æ‰¹æ ·æœ¬æ•° (é»˜è®¤16): ") or "16")
                num_batches = int(input("æ‰¹æ¬¡æ•°é‡ (é»˜è®¤5): ") or "5")
                
                os.makedirs(f"batch_generation_{image_size}x{image_size}", exist_ok=True)
                
                for i in range(num_batches):
                    output_path = f"batch_generation_{image_size}x{image_size}/batch_{i+1}.png"
                    generate_high_res_samples(model, num_samples=batch_size, save_path=output_path)
                    print(f"âœ… æ‰¹æ¬¡ {i+1}/{num_batches} å®Œæˆ")
                
                print("ğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆï¼")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except (ValueError, KeyboardInterrupt):
        print("\nğŸ‘‹ é€€å‡ºç¨‹åº") 