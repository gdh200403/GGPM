import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
from config import get_auto_config, print_config # å¯¼å…¥é…ç½®

def test_train_setup():
    """æµ‹è¯•è®­ç»ƒè®¾ç½®æ˜¯å¦æ­£å¸¸"""
    print("ğŸ” éªŒè¯è®­ç»ƒè„šæœ¬è®¾ç½®...")
    
    try:
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        dataloader, dataset = get_cifar10_dataloader(batch_size=4, image_size=32)
        print(f"âœ… æ•°æ®åŠ è½½æ­£å¸¸ - æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        model = DDPMModel(
            image_size=32,
            channels=3,
            dim=32,  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
            dim_mults=(1, 2),
            timesteps=100
        )
        print("âœ… æ¨¡å‹åˆ›å»ºæ­£å¸¸")
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒ
        data_iter = iter(dataloader)
        test_batch, _ = next(data_iter)
        
        optimizer = torch.optim.Adam(model.unet.parameters(), lr=1e-4)
        
        model.unet.train()
        optimizer.zero_grad()
        loss = model.train_step(test_batch)
        loss.backward()
        optimizer.step()
        
        print(f"âœ… è®­ç»ƒæ­¥éª¤æ­£å¸¸ - æµ‹è¯•æŸå¤±: {loss.item():.4f}")
        print("ğŸ‰ è®­ç»ƒè„šæœ¬éªŒè¯é€šè¿‡ï¼\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè„šæœ¬éªŒè¯å¤±è´¥: {e}")
        return False

def get_cifar10_dataloader(batch_size=32, image_size=32, num_workers=2, pin_memory=True): # ä»configè·å–é»˜è®¤å€¼
    """è·å–CIFAR-10æ•°æ®åŠ è½½å™¨"""
    transform = transforms.Compose([
        transforms.Resize(image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1, 1]
    ])
    
    dataset = torchvision.datasets.CIFAR10(
        root='./data', # å¯ä»¥ä»configè·å–
        train=True,
        download=True,
        transform=transform
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    return dataloader, dataset

def train_ddpm(config): # æ¥æ”¶é…ç½®å¯¹è±¡ä½œä¸ºå‚æ•°
    """è®­ç»ƒDDPMæ¨¡å‹"""
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=config.IMAGE_SIZE,
        channels=config.CHANNELS,
        dim=config.DIM,
        dim_mults=config.DIM_MULTS,
        timesteps=config.TIMESTEPS
    )
    model.unet.to(config.DEVICE) # å°†æ¨¡å‹çš„unetéƒ¨åˆ†ç§»åŠ¨åˆ°é…ç½®çš„è®¾å¤‡
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.unet.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    
    # è·å–æ•°æ®åŠ è½½å™¨
    dataloader, dataset = get_cifar10_dataloader(
        batch_size=config.BATCH_SIZE,
        image_size=config.IMAGE_SIZE,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(config.SAMPLE_DIR, exist_ok=True)
    
    # è®­ç»ƒå†å²è®°å½•
    losses = []
    
    print(f"å¼€å§‹è®­ç»ƒDDPMæ¨¡å‹...")
    print_config(config) # æ‰“å°é…ç½®ä¿¡æ¯
        
    for epoch in range(config.EPOCHS):
        model.unet.train()
        epoch_losses = []
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{config.EPOCHS}')
        for batch_idx, (data, _) in enumerate(pbar):
            data = data.to(config.DEVICE) # å°†æ•°æ®ç§»åŠ¨åˆ°é…ç½®çš„è®¾å¤‡
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            loss = model.train_step(data)
            
            # åå‘ä¼ æ’­
            loss.backward()
            if config.GRADIENT_CLIP > 0:
                 torch.nn.utils.clip_grad_norm_(model.unet.parameters(), config.GRADIENT_CLIP)
            optimizer.step()
            
            epoch_losses.append(loss.item())
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # è®°å½•å¹³å‡æŸå¤±
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'Epoch {epoch+1}, å¹³å‡æŸå¤±: {avg_loss:.4f}')
        
        # ä¿å­˜æ£€æŸ¥ç‚¹å’Œç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % config.SAVE_INTERVAL == 0:
            # ä¿å­˜æ¨¡å‹
            checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'ddpm_epoch_{epoch+1}.pt')
            model.save_model(checkpoint_path)
            
            # ç”Ÿæˆæ ·æœ¬å¹¶ä¿å­˜
            model.unet.eval()
            with torch.no_grad():
                samples = model.sample(batch_size=config.INFERENCE_BATCH_SIZE) # ä½¿ç”¨é…ç½®çš„æ¨ç†æ‰¹æ¬¡å¤§å°å’Œè®¾å¤‡
                save_samples(samples, os.path.join(config.SAMPLE_DIR, f'epoch_{epoch+1}_samples.png'))
            
            # ä¿å­˜æŸå¤±æ›²çº¿
            plot_losses(losses, os.path.join(config.SAMPLE_DIR, f'loss_curve_epoch_{epoch+1}.png'))
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(config.CHECKPOINT_DIR, 'ddpm_final.pt')
    model.save_model(final_path)
    
    print("è®­ç»ƒå®Œæˆï¼")
    return model, losses

def save_samples(samples, path):
    """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬å›¾åƒ"""
    try:
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # å°†æ ·æœ¬ä»[-1, 1]è½¬æ¢åˆ°[0, 1]
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ
        grid = torchvision.utils.make_grid(samples, nrow=4, padding=2)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶ä¿å­˜
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        plt.figure(figsize=(10, 10))
        plt.imshow(grid_np)
        plt.axis('off')
        plt.title('Generated Samples')
        plt.savefig(path, bbox_inches='tight', dpi=150)
        plt.close()
        
        print(f"æ ·æœ¬å·²ä¿å­˜åˆ°: {path}")
    except Exception as e:
        print(f"ä¿å­˜æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_losses(losses, path):
    """ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿"""
    try:
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(10, 6))
        plt.plot(losses)
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.savefig(path, bbox_inches='tight', dpi=150)
        plt.close()
        
        print(f"æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {path}")
    except Exception as e:
        print(f"ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    # è·å–é…ç½®
    config = get_auto_config()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        # åªè¿è¡ŒéªŒè¯æµ‹è¯•
        # æ³¨æ„ï¼štest_train_setup ä¹Ÿéœ€è¦è°ƒæ•´ä»¥æ¥å—æˆ–ä½¿ç”¨config
        if test_train_setup(): # ä½ å¯èƒ½éœ€è¦ä¿®æ”¹ test_train_setup ä»¥ä½¿ç”¨ config
            print("âœ… è®­ç»ƒè„šæœ¬å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®Œæ•´è®­ç»ƒ")
        else:
            print("âŒ è¯·ä¿®å¤é—®é¢˜åå†å¼€å§‹è®­ç»ƒ")
        sys.exit()
    
    # è¿è¡ŒéªŒè¯æµ‹è¯•
    print("å¼€å§‹è®­ç»ƒå‰éªŒè¯...")
    # æ³¨æ„ï¼štest_train_setup ä¹Ÿéœ€è¦è°ƒæ•´ä»¥æ¥å—æˆ–ä½¿ç”¨config
    if not test_train_setup(): # ä½ å¯èƒ½éœ€è¦ä¿®æ”¹ test_train_setup ä»¥ä½¿ç”¨ config
        print("âŒ éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒè®¾ç½®")
        sys.exit(1)
    
    # å¼€å§‹å®Œæ•´è®­ç»ƒ
    try:
        model, losses = train_ddpm(config) # ä¼ é€’é…ç½®å¯¹è±¡
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()