import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time

# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False

def get_optimized_cifar10_dataloader(batch_size=32, image_size=32, num_workers=None):
    """è·å–ä¼˜åŒ–çš„CIFAR-10æ•°æ®åŠ è½½å™¨"""
    
    # è‡ªåŠ¨è®¾ç½®æœ€ä¼˜çš„workeræ•°é‡
    if num_workers is None:
        num_workers = min(8, os.cpu_count() or 4)  # é»˜è®¤4ä¸ªworkerï¼Œé¿å…None
    
    # ä¼˜åŒ–çš„æ•°æ®å˜æ¢ï¼ˆå‡å°‘ä¸å¿…è¦çš„æ“ä½œï¼‰
    transform = transforms.Compose([
        transforms.ToTensor(),  # ç›´æ¥è½¬æ¢ï¼ŒCIFAR-10å·²ç»æ˜¯32x32
        transforms.RandomHorizontalFlip(p=0.5),  # æ˜ç¡®æ¦‚ç‡
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )
    
    # ä¼˜åŒ–çš„DataLoaderè®¾ç½®
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,  # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=2,  # é¢„å–å› å­
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batchï¼Œä¿æŒä¸€è‡´æ€§
    )
    
    return dataloader, dataset

def train_ddpm_optimized(
    epochs=500,  # å¢åŠ åˆ°500è½®ä»¥è·å¾—æ›´å¥½æ•ˆæœ
    batch_size=32, 
    learning_rate=1e-4, 
    save_interval=25,  # è°ƒæ•´ä¿å­˜é—´éš”ï¼Œé¿å…é¢‘ç¹ä¿å­˜
    use_amp=True,  # æ··åˆç²¾åº¦è®­ç»ƒ
    gradient_accumulation_steps=1,  # æ¢¯åº¦ç´¯ç§¯
    compile_model=True,  # æ¨¡å‹ç¼–è¯‘åŠ é€Ÿ
    efficient_checkpointing=True,  # é«˜æ•ˆæ£€æŸ¥ç‚¹
    fast_sampling_interval=10  # è°ƒæ•´å¿«é€Ÿé‡‡æ ·é—´éš”
):
    """ä¼˜åŒ–ç‰ˆæœ¬çš„DDPMè®­ç»ƒ"""
    
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆæœ¬DDPMè®­ç»ƒ")
    print(f"âš¡ ä¼˜åŒ–ç‰¹æ€§:")
    print(f"   - æ··åˆç²¾åº¦è®­ç»ƒ: {'âœ…' if use_amp and AMP_AVAILABLE else 'âŒ'}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯: {'âœ…' if gradient_accumulation_steps > 1 else 'âŒ'}")
    print(f"   - æ¨¡å‹ç¼–è¯‘: {'âœ…' if compile_model else 'âŒ'}")
    print(f"   - é«˜æ•ˆæ£€æŸ¥ç‚¹: {'âœ…' if efficient_checkpointing else 'âŒ'}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=32,
        channels=3,
        dim=64,
        dim_mults=(1, 2, 4, 8),
        timesteps=1000
    )
    
    # æ¨¡å‹ç¼–è¯‘åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
    if compile_model and hasattr(torch, 'compile'):
        try:
            compiled_unet = torch.compile(model.unet)
            model.unet = compiled_unet  # type: ignore
            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨AdamWå’Œæ›´å¥½çš„å‚æ•°ï¼‰
    optimizer = optim.AdamW(
        model.unet.parameters(), 
        lr=learning_rate,
        weight_decay=0.01,  # æƒé‡è¡°å‡
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - é’ˆå¯¹é•¿è®­ç»ƒä¼˜åŒ–
    # å‰10%çš„epochç”¨äºé¢„çƒ­ï¼Œç„¶åä½¿ç”¨ä½™å¼¦é€€ç«
    warmup_epochs = max(1, epochs // 10)  # é¢„çƒ­epochæ•°
    
    # ä½¿ç”¨æ›´å¹³æ»‘çš„å­¦ä¹ ç‡è°ƒåº¦
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=epochs - warmup_epochs,  # é¢„çƒ­åçš„è®­ç»ƒè½®æ•°
        eta_min=learning_rate * 0.01  # æ›´ä½çš„æœ€å°å­¦ä¹ ç‡
    )
    
    # é¢„çƒ­è°ƒåº¦å™¨ï¼ˆç®€å•çš„çº¿æ€§é¢„çƒ­ï¼‰
    warmup_scheduler = optim.lr_scheduler.LinearLR(
        optimizer,
        start_factor=0.1,  # ä»10%çš„å­¦ä¹ ç‡å¼€å§‹
        total_iters=warmup_epochs
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if use_amp and AMP_AVAILABLE else None
    
    # è·å–ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    effective_batch_size = batch_size * gradient_accumulation_steps
    dataloader, dataset = get_optimized_cifar10_dataloader(
        batch_size=batch_size, 
        num_workers=8  # å¢åŠ workeræ•°é‡
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('samples', exist_ok=True)
    os.makedirs('optimized_training', exist_ok=True)
    
    # è®­ç»ƒå†å²è®°å½•
    losses = []
    best_loss = float('inf')
    patience_counter = 0  # ç”¨äºæ—©åœçš„è®¡æ•°å™¨
    patience = epochs // 4  # è€å¿ƒå€¼è®¾ä¸ºæ€»è½®æ•°çš„1/4ï¼Œå®é™…ä¸Šå¯¹DDPMä¸å»ºè®®ä½¿ç”¨
    
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   - è®­ç»ƒè½®æ•°: {epochs} (é¢„è®¡è€—æ—¶: ~{epochs * len(dataloader) * batch_size / 50000 * 2:.1f}å°æ—¶)")
    print(f"   - å®é™…æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    print(f"   - åˆå§‹å­¦ä¹ ç‡: {learning_rate}")
    print(f"   - é¢„çƒ­è½®æ•°: {warmup_epochs}")
    print(f"   - æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   - é¢„è®¡æ¯epochæ‰¹æ¬¡æ•°: {len(dataloader)}")
    print(f"   - ä¿å­˜é—´éš”: æ¯{save_interval}è½®")
    print(f"   - é‡‡æ ·é—´éš”: æ¯{fast_sampling_interval}è½®")
    print("=" * 60)
    
    # å¼€å§‹è®­ç»ƒ
    training_start_time = time.time()
    
    for epoch in range(epochs):
        model.unet.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        # é‡ç½®æ¢¯åº¦ç´¯ç§¯
        optimizer.zero_grad()
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if use_amp and scaler is not None:
                with autocast():
                    loss = model.train_step(data) / gradient_accumulation_steps
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                # æ ‡å‡†è®­ç»ƒ
                loss = model.train_step(data) / gradient_accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
            
            epoch_losses.append(loss.item() * gradient_accumulation_steps)
            
            # æ›´æ–°è¿›åº¦æ¡
            if epoch < warmup_epochs:
                current_lr = warmup_scheduler.get_last_lr()[0]
            else:
                current_lr = scheduler.get_last_lr()[0]
            pbar.set_postfix({
                'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                'lr': f'{current_lr:.2e}'
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦ - åŒºåˆ†é¢„çƒ­æœŸå’Œæ­£å¸¸è®­ç»ƒæœŸ
        if epoch < warmup_epochs:
            warmup_scheduler.step()
        else:
            scheduler.step()
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'ğŸ“Š Epoch {epoch+1}/{epochs}:')
        print(f'   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’')
        print(f'   ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'   ğŸ“ˆ å­¦ä¹ ç‡: {current_lr:.2e}')
        if epoch < warmup_epochs:
            print(f'   ğŸ”¥ é¢„çƒ­é˜¶æ®µ: {epoch+1}/{warmup_epochs}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0  # é‡ç½®è€å¿ƒè®¡æ•°å™¨
            if efficient_checkpointing:
                best_path = 'checkpoints/ddpm_best.pt'
                model.save_model(best_path)
                print(f'   ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (æ”¹è¿›: {(losses[-2] - avg_loss) if len(losses) > 1 else 0:.4f})')
        else:
            patience_counter += 1
        
        # å¿«é€Ÿé‡‡æ ·ç›‘æ§
        if (epoch + 1) % fast_sampling_interval == 0:
            model.unet.eval()
            with torch.no_grad():
                # æ›´å¿«çš„é‡‡æ ·ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ï¼‰
                quick_samples = model.sample(batch_size=4)
                save_optimized_samples(
                    quick_samples, 
                    f'optimized_training/quick_epoch_{epoch+1}.png',
                    epoch=epoch+1,
                    loss=avg_loss
                )
            print(f'   ğŸ“¸ å¿«é€Ÿé‡‡æ ·å·²ä¿å­˜')
        
        # å®Œæ•´æ£€æŸ¥ç‚¹ä¿å­˜
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoints/ddpm_epoch_{epoch+1}.pt'
            model.save_model(checkpoint_path)
            
            # ç”Ÿæˆå®Œæ•´æ ·æœ¬
            model.unet.eval()
            with torch.no_grad():
                samples = model.sample(batch_size=16)
                save_optimized_samples(
                    samples, 
                    f'samples/epoch_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss
                )
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            plot_optimized_losses(losses, f'samples/loss_curve_epoch_{epoch+1}.png')
            print(f'   ğŸ’¾ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜')
        
        # é‡è¦é‡Œç¨‹ç¢‘ä¿å­˜ï¼ˆ100, 200, 300, 400è½®ï¼‰
        if (epoch + 1) in [100, 200, 300, 400]:
            milestone_path = f'checkpoints/ddpm_milestone_{epoch+1}.pt'
            model.save_model(milestone_path)
            
            # ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬è¿›è¡Œé‡Œç¨‹ç¢‘è¯„ä¼°
            model.unet.eval()
            with torch.no_grad():
                milestone_samples = model.sample(batch_size=25)  # æ›´å¤šæ ·æœ¬
                save_optimized_samples(
                    milestone_samples, 
                    f'samples/milestone_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss
                )
            print(f'   ğŸ é‡Œç¨‹ç¢‘ {epoch+1} è½®æ¨¡å‹å·²ä¿å­˜')
        
        print("-" * 50)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - training_start_time
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = 'checkpoints/ddpm_final_optimized.pt'
    model.save_model(final_path)
    
    print("ğŸ‰ é•¿æœŸä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“‰ æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {epochs} è½®")
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    print(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: checkpoints/")
    print(f"ğŸ–¼ï¸  æ ·æœ¬ç›®å½•: samples/")
    print(f"âš¡ å¹³å‡æ¯è½®ç”¨æ—¶: {total_time/epochs:.1f}ç§’")
    
    return model, losses

def save_optimized_samples(samples, path, epoch=None, loss=None):
    """ä¼˜åŒ–çš„æ ·æœ¬ä¿å­˜å‡½æ•°"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        grid = torchvision.utils.make_grid(samples, nrow=4, padding=2)
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        plt.figure(figsize=(10, 8))
        plt.imshow(grid_np)
        plt.axis('off')
        
        if epoch is not None and loss is not None:
            plt.title(f'Epoch {epoch} - Loss: {loss:.4f}', fontsize=14, fontweight='bold')
        
        plt.savefig(path, bbox_inches='tight', dpi=100)  # é™ä½DPIåŠ é€Ÿä¿å­˜
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_optimized_losses(losses, path):
    """ä¼˜åŒ–çš„æŸå¤±ç»˜åˆ¶å‡½æ•°"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(10, 6))
        plt.plot(losses, linewidth=2)
        plt.title('Training Loss (Optimized)', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æœ€å°å€¼æ ‡è®°
        min_loss_idx = np.argmin(losses)
        min_loss = losses[min_loss_idx]
        plt.scatter([min_loss_idx], [min_loss], color='red', s=100, zorder=5)
        plt.annotate(f'Best: {min_loss:.4f}', 
                    xy=(float(min_loss_idx), float(min_loss)),
                    xytext=(10, 10), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))
        
        plt.savefig(path, bbox_inches='tight', dpi=100)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    use_amp = '--no-amp' not in sys.argv
    compile_model = '--no-compile' not in sys.argv
    
    print("ğŸš€ å¯åŠ¨é•¿æœŸä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    print(f"æ··åˆç²¾åº¦: {'å¼€å¯' if use_amp else 'å…³é—­'}")
    print(f"æ¨¡å‹ç¼–è¯‘: {'å¼€å¯' if compile_model else 'å…³é—­'}")
    print("ğŸ“ è®­ç»ƒé…ç½®è¯´æ˜:")
    print("   - å¢åŠ åˆ°500è½®è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡")
    print("   - æ·»åŠ å­¦ä¹ ç‡é¢„çƒ­æœºåˆ¶æé«˜è®­ç»ƒç¨³å®šæ€§")
    print("   - è®¾ç½®é‡Œç¨‹ç¢‘ä¿å­˜ç‚¹ä¾¿äºç›‘æ§é•¿æœŸè®­ç»ƒæ•ˆæœ")
    print("   - è°ƒæ•´ä¿å­˜å’Œé‡‡æ ·é—´éš”å‡å°‘IOå¼€é”€")
    
    try:
        # é’ˆå¯¹ä¸åŒGPUä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if '3090' in gpu_name or '4090' in gpu_name:
                batch_size = 32
                gradient_accumulation = 1
            elif '3060' in gpu_name:
                batch_size = 16
                gradient_accumulation = 2  # é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§batch
            else:
                batch_size = 16
                gradient_accumulation = 1
        else:
            batch_size = 8
            gradient_accumulation = 1
        
        model, losses = train_ddpm_optimized(
            epochs=250,  # å¢åŠ è®­ç»ƒè½®æ•°è·å¾—æ›´å¥½æ•ˆæœ
            batch_size=batch_size,
            learning_rate=1e-4,  # ç¨å¾®æé«˜å­¦ä¹ ç‡
            save_interval=25,  # è°ƒæ•´ä¿å­˜é—´éš”
            use_amp=use_amp,
            gradient_accumulation_steps=gradient_accumulation,
            compile_model=compile_model,
            fast_sampling_interval=10  # è°ƒæ•´é‡‡æ ·é—´éš”
        )
        
        print("ğŸ‰ ä¼˜åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 