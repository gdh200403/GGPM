import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time

# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False

def get_high_res_dataloader(
    dataset_name='cifar10', 
    image_size=64, 
    batch_size=16, 
    num_workers=None
):
    """è·å–é«˜åˆ†è¾¨ç‡æ•°æ®åŠ è½½å™¨"""
    
    if num_workers is None:
        num_workers = min(8, os.cpu_count() or 4)
    
    # åŸºç¡€å˜æ¢
    base_transforms = [
        transforms.Resize(image_size),
        transforms.CenterCrop(image_size),
    ]
    
    # æ•°æ®å¢å¼ºï¼ˆæ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´ï¼‰
    augment_transforms = []
    if image_size >= 64:
        augment_transforms.extend([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),  # é«˜åˆ†è¾¨ç‡å¯ä»¥åŠ æ›´å¤šå¢å¼º
        ])
    else:
        augment_transforms.append(transforms.RandomHorizontalFlip(p=0.5))
    
    # æœ€ç»ˆå˜æ¢
    final_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]
    
    transform = transforms.Compose(base_transforms + augment_transforms + final_transforms)
    
    # é€‰æ‹©æ•°æ®é›†
    if dataset_name.lower() == 'cifar10':
        dataset = torchvision.datasets.CIFAR10(
            root='./data',
            train=True,
            download=True,
            transform=transform
        )
    elif dataset_name.lower() == 'cifar100':
        dataset = torchvision.datasets.CIFAR100(
            root='./data',
            train=True,
            download=True,
            transform=transform
        )
    elif dataset_name.lower() == 'celeba':
        # CelebAæ•°æ®é›†ï¼ˆéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼‰
        try:
            dataset = torchvision.datasets.CelebA(
                root='./data',
                split='train',
                download=False,  # é€šå¸¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½
                transform=transform
            )
        except Exception as e:
            print(f"âŒ CelebAæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            print("è¯·æ‰‹åŠ¨ä¸‹è½½CelebAæ•°æ®é›†æˆ–ä½¿ç”¨å…¶ä»–æ•°æ®é›†")
            raise
    elif dataset_name.lower() == 'imagenet':
        # ImageNetçš„å°ç‰ˆæœ¬æˆ–ä½¿ç”¨STL10ä½œä¸ºæ›¿ä»£
        try:
            dataset = torchvision.datasets.STL10(
                root='./data',
                split='train',
                download=True,
                transform=transform
            )
            print("ä½¿ç”¨STL10æ•°æ®é›†ä½œä¸ºé«˜åˆ†è¾¨ç‡æ›¿ä»£")
        except Exception as e:
            print(f"âŒ é«˜åˆ†è¾¨ç‡æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=2,
        drop_last=True
    )
    
    return dataloader, dataset

def get_model_config_for_resolution(image_size):
    """æ ¹æ®å›¾åƒåˆ†è¾¨ç‡è·å–æ¨èçš„æ¨¡å‹é…ç½®"""
    if image_size <= 32:
        return {
            'dim': 64,
            'dim_mults': (1, 2, 4, 8),
            'timesteps': 1000
        }
    elif image_size <= 64:
        return {
            'dim': 128,
            'dim_mults': (1, 2, 4, 8),
            'timesteps': 1000
        }
    elif image_size <= 128:
        return {
            'dim': 192,
            'dim_mults': (1, 1, 2, 2, 4, 4),
            'timesteps': 1000
        }
    elif image_size <= 256:
        return {
            'dim': 256,
            'dim_mults': (1, 1, 2, 2, 4, 4, 8),
            'timesteps': 1000
        }
    else:
        return {
            'dim': 320,
            'dim_mults': (1, 1, 2, 2, 4, 4, 8, 8),
            'timesteps': 1000
        }

def get_batch_size_for_resolution(image_size, gpu_memory_gb=12):
    """æ ¹æ®åˆ†è¾¨ç‡å’ŒGPUå†…å­˜æ¨èæ‰¹æ¬¡å¤§å°"""
    if image_size <= 32:
        if gpu_memory_gb >= 24:
            return 64
        elif gpu_memory_gb >= 12:
            return 32
        else:
            return 16
    elif image_size <= 64:
        if gpu_memory_gb >= 24:
            return 32
        elif gpu_memory_gb >= 12:
            return 16
        else:
            return 8
    elif image_size <= 128:
        if gpu_memory_gb >= 24:
            return 16
        elif gpu_memory_gb >= 12:
            return 8
        else:
            return 4
    elif image_size <= 256:
        if gpu_memory_gb >= 24:
            return 8
        elif gpu_memory_gb >= 12:
            return 4
        else:
            return 2
    else:  # 512+
        if gpu_memory_gb >= 24:
            return 4
        elif gpu_memory_gb >= 12:
            return 2
        else:
            return 1

def train_ddpm_high_res(
    dataset_name='cifar10',
    image_size=64,
    epochs=50,
    batch_size=None,  # è‡ªåŠ¨æ ¹æ®åˆ†è¾¨ç‡è®¡ç®—
    learning_rate=1e-4,
    save_interval=10,
    use_amp=True,
    gradient_accumulation_steps=None,  # è‡ªåŠ¨è®¡ç®—
    compile_model=True,
    fast_sampling_interval=5
):
    """é«˜åˆ†è¾¨ç‡DDPMè®­ç»ƒ"""
    
    print(f"ğŸš€ å¯åŠ¨é«˜åˆ†è¾¨ç‡DDPMè®­ç»ƒ")
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   - æ•°æ®é›†: {dataset_name.upper()}")
    print(f"   - å›¾åƒåˆ†è¾¨ç‡: {image_size}x{image_size}")
    
    # è·å–GPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        # ä¼°ç®—GPUå†…å­˜
        if '3060' in gpu_name:
            gpu_memory = 12
        elif '3090' in gpu_name or '4090' in gpu_name:
            gpu_memory = 24
        elif '3080' in gpu_name:
            gpu_memory = 10
        else:
            gpu_memory = 8  # ä¿å®ˆä¼°è®¡
        print(f"   - GPU: {gpu_name} ({gpu_memory}GB)")
    else:
        gpu_memory = 0
        print("   - è®¾å¤‡: CPU")
    
    # è‡ªåŠ¨é…ç½®æ‰¹æ¬¡å¤§å°
    if batch_size is None:
        batch_size = get_batch_size_for_resolution(image_size, gpu_memory)
    
    # è‡ªåŠ¨é…ç½®æ¢¯åº¦ç´¯ç§¯
    if gradient_accumulation_steps is None:
        # å°è¯•ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°åœ¨16-32ä¹‹é—´
        target_effective_batch = 24
        gradient_accumulation_steps = max(1, target_effective_batch // batch_size)
    
    effective_batch_size = batch_size * gradient_accumulation_steps
    
    print(f"   - å®é™…æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = get_model_config_for_resolution(image_size)
    print(f"   - æ¨¡å‹ç»´åº¦: {model_config['dim']}")
    print(f"   - å±‚çº§å€æ•°: {model_config['dim_mults']}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=image_size,
        channels=3,
        **model_config
    )
    
    # æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.unet.parameters())
    print(f"   - æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # æ¨¡å‹ç¼–è¯‘åŠ é€Ÿ
    if compile_model and hasattr(torch, 'compile'):
        try:
            compiled_unet = torch.compile(model.unet)
            model.unet = compiled_unet  # type: ignore
            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
    
    # ä¼˜åŒ–å™¨è®¾ç½®ï¼ˆå¯¹é«˜åˆ†è¾¨ç‡è°ƒæ•´å­¦ä¹ ç‡ï¼‰
    if image_size > 64:
        learning_rate = learning_rate * 0.8  # é«˜åˆ†è¾¨ç‡ä½¿ç”¨ç¨ä½çš„å­¦ä¹ ç‡
    
    optimizer = optim.AdamW(
        model.unet.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=epochs,
        eta_min=learning_rate * 0.1
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if use_amp and AMP_AVAILABLE else None
    
    # è·å–æ•°æ®åŠ è½½å™¨
    dataloader, dataset = get_high_res_dataloader(
        dataset_name=dataset_name,
        image_size=image_size,
        batch_size=batch_size,
        num_workers=8
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    exp_name = f"{dataset_name}_{image_size}x{image_size}"
    os.makedirs(f'checkpoints/{exp_name}', exist_ok=True)
    os.makedirs(f'samples/{exp_name}', exist_ok=True)
    os.makedirs(f'training_progress/{exp_name}', exist_ok=True)
    
    print(f"ğŸ“ å®éªŒåç§°: {exp_name}")
    print("=" * 60)
    
    # è®­ç»ƒå†å²è®°å½•
    losses = []
    best_loss = float('inf')
    
    # å¼€å§‹è®­ç»ƒ
    training_start_time = time.time()
    
    for epoch in range(epochs):
        model.unet.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        optimizer.zero_grad()
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if use_amp and scaler is not None:
                with autocast():
                    loss = model.train_step(data) / gradient_accumulation_steps
                
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss = model.train_step(data) / gradient_accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
            
            epoch_losses.append(loss.item() * gradient_accumulation_steps)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = scheduler.get_last_lr()[0]
            pbar.set_postfix({
                'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                'lr': f'{current_lr:.2e}'
            })
        
        scheduler.step()
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'ğŸ“Š Epoch {epoch+1}/{epochs}:')
        print(f'   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’')
        print(f'   ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'   ğŸ“ˆ å­¦ä¹ ç‡: {current_lr:.2e}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = f'checkpoints/{exp_name}/ddpm_best.pt'
            model.save_model(best_path)
            print(f'   ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜')
        
        # å¿«é€Ÿé‡‡æ ·ç›‘æ§ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´ï¼‰
        if (epoch + 1) % fast_sampling_interval == 0:
            model.unet.eval()
            with torch.no_grad():
                # é«˜åˆ†è¾¨ç‡ç”¨æ›´å°‘çš„æ ·æœ¬
                sample_count = max(4, 16 // (image_size // 32))
                quick_samples = model.sample(batch_size=sample_count)
                save_high_res_samples(
                    quick_samples,
                    f'training_progress/{exp_name}/quick_epoch_{epoch+1}.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    image_size=image_size
                )
            print(f'   ğŸ“¸ å¿«é€Ÿé‡‡æ ·å·²ä¿å­˜')
        
        # å®Œæ•´æ£€æŸ¥ç‚¹ä¿å­˜
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoints/{exp_name}/ddpm_epoch_{epoch+1}.pt'
            model.save_model(checkpoint_path)
            
            # ç”Ÿæˆå®Œæ•´æ ·æœ¬
            model.unet.eval()
            with torch.no_grad():
                sample_count = max(8, 16 // (image_size // 32))
                samples = model.sample(batch_size=sample_count)
                save_high_res_samples(
                    samples,
                    f'samples/{exp_name}/epoch_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    image_size=image_size
                )
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            plot_high_res_losses(losses, f'samples/{exp_name}/loss_curve_epoch_{epoch+1}.png', exp_name)
            print(f'   ğŸ’¾ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜')
        
        print("-" * 50)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - training_start_time
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = f'checkpoints/{exp_name}/ddpm_final.pt'
    model.save_model(final_path)
    
    print("ğŸ‰ é«˜åˆ†è¾¨ç‡è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸ“‰ æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    
    return model, losses

def save_high_res_samples(samples, path, epoch=None, loss=None, image_size=64):
    """ä¿å­˜é«˜åˆ†è¾¨ç‡æ ·æœ¬"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        # æ ¹æ®å›¾åƒå¤§å°è°ƒæ•´ç½‘æ ¼
        if len(samples) <= 4:
            nrow = 2
        elif len(samples) <= 9:
            nrow = 3
        else:
            nrow = 4
        
        grid = torchvision.utils.make_grid(samples, nrow=nrow, padding=2)
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        # æ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´å›¾åƒå¤§å°
        fig_size = min(12, max(8, image_size // 16))
        plt.figure(figsize=(fig_size, fig_size))
        plt.imshow(grid_np)
        plt.axis('off')
        
        if epoch is not None and loss is not None:
            plt.title(f'Epoch {epoch} - Loss: {loss:.4f} ({image_size}x{image_size})', 
                     fontsize=14, fontweight='bold')
        
        # é«˜åˆ†è¾¨ç‡å›¾åƒä¿å­˜æ›´é«˜è´¨é‡
        dpi = 150 if image_size >= 128 else 100
        plt.savefig(path, bbox_inches='tight', dpi=dpi)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_high_res_losses(losses, path, exp_name):
    """ç»˜åˆ¶é«˜åˆ†è¾¨ç‡è®­ç»ƒæŸå¤±"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(12, 6))
        plt.plot(losses, linewidth=2, color='blue')
        plt.title(f'Training Loss - {exp_name}', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æœ€å°å€¼æ ‡è®°
        if losses:
            min_loss_idx = np.argmin(losses)
            min_loss = losses[min_loss_idx]
            plt.scatter([min_loss_idx], [min_loss], color='red', s=100, zorder=5)
            plt.annotate(f'Best: {min_loss:.4f}', 
                        xy=(float(min_loss_idx), float(min_loss)),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),
                        fontsize=10)
        
        plt.tight_layout()
        plt.savefig(path, bbox_inches='tight', dpi=150)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    use_amp = '--no-amp' not in sys.argv
    compile_model = '--no-compile' not in sys.argv
    
    # è§£ææ•°æ®é›†å’Œåˆ†è¾¨ç‡
    dataset_name = 'cifar10'
    image_size = 64
    
    for arg in sys.argv[1:]:
        if arg.startswith('--dataset='):
            dataset_name = arg.split('=')[1]
        elif arg.startswith('--size='):
            image_size = int(arg.split('=')[1])
    
    print("ğŸš€ å¯åŠ¨é«˜åˆ†è¾¨ç‡è®­ç»ƒè„šæœ¬")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"åˆ†è¾¨ç‡: {image_size}x{image_size}")
    print(f"æ··åˆç²¾åº¦: {'å¼€å¯' if use_amp else 'å…³é—­'}")
    print(f"æ¨¡å‹ç¼–è¯‘: {'å¼€å¯' if compile_model else 'å…³é—­'}")
    print("\nå¯ç”¨é€‰é¡¹:")
    print("  --dataset=cifar10/cifar100/celeba/imagenet")
    print("  --size=32/64/128/256")
    print("  --no-amp (å…³é—­æ··åˆç²¾åº¦)")
    print("  --no-compile (å…³é—­æ¨¡å‹ç¼–è¯‘)")
    
    try:
        model, losses = train_ddpm_high_res(
            dataset_name=dataset_name,
            image_size=image_size,
            epochs=50,
            use_amp=use_amp,
            compile_model=compile_model
        )
        
        print("ğŸ‰ é«˜åˆ†è¾¨ç‡è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 