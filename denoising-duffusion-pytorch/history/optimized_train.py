import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time

# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False

def get_optimized_cifar10_dataloader(batch_size=32, image_size=32, num_workers=None):
    """è·å–ä¼˜åŒ–çš„CIFAR-10æ•°æ®åŠ è½½å™¨"""
    
    # è‡ªåŠ¨è®¾ç½®æœ€ä¼˜çš„workeræ•°é‡
    if num_workers is None:
        num_workers = min(8, os.cpu_count() or 4)  # é»˜è®¤4ä¸ªworkerï¼Œé¿å…None
    
    # ä¼˜åŒ–çš„æ•°æ®å˜æ¢ï¼ˆå‡å°‘ä¸å¿…è¦çš„æ“ä½œï¼‰
    transform = transforms.Compose([
        transforms.ToTensor(),  # ç›´æ¥è½¬æ¢ï¼ŒCIFAR-10å·²ç»æ˜¯32x32
        transforms.RandomHorizontalFlip(p=0.5),  # æ˜ç¡®æ¦‚ç‡
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )
    
    # ä¼˜åŒ–çš„DataLoaderè®¾ç½®
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,  # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=2,  # é¢„å–å› å­
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batchï¼Œä¿æŒä¸€è‡´æ€§
    )
    
    return dataloader, dataset

def train_ddpm_optimized(
    epochs=50, 
    batch_size=32, 
    learning_rate=1e-4, 
    save_interval=10,
    use_amp=True,  # æ··åˆç²¾åº¦è®­ç»ƒ
    gradient_accumulation_steps=1,  # æ¢¯åº¦ç´¯ç§¯
    compile_model=True,  # æ¨¡å‹ç¼–è¯‘åŠ é€Ÿ
    efficient_checkpointing=True,  # é«˜æ•ˆæ£€æŸ¥ç‚¹
    fast_sampling_interval=5  # å¿«é€Ÿé‡‡æ ·é—´éš”
):
    """ä¼˜åŒ–ç‰ˆæœ¬çš„DDPMè®­ç»ƒ"""
    
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆæœ¬DDPMè®­ç»ƒ")
    print(f"âš¡ ä¼˜åŒ–ç‰¹æ€§:")
    print(f"   - æ··åˆç²¾åº¦è®­ç»ƒ: {'âœ…' if use_amp and AMP_AVAILABLE else 'âŒ'}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯: {'âœ…' if gradient_accumulation_steps > 1 else 'âŒ'}")
    print(f"   - æ¨¡å‹ç¼–è¯‘: {'âœ…' if compile_model else 'âŒ'}")
    print(f"   - é«˜æ•ˆæ£€æŸ¥ç‚¹: {'âœ…' if efficient_checkpointing else 'âŒ'}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=32,
        channels=3,
        dim=64,
        dim_mults=(1, 2, 4, 8),
        timesteps=1000
    )
    
    # æ¨¡å‹ç¼–è¯‘åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
    if compile_model and hasattr(torch, 'compile'):
        try:
            compiled_unet = torch.compile(model.unet)
            model.unet = compiled_unet  # type: ignore
            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨AdamWå’Œæ›´å¥½çš„å‚æ•°ï¼‰
    optimizer = optim.AdamW(
        model.unet.parameters(), 
        lr=learning_rate,
        weight_decay=0.01,  # æƒé‡è¡°å‡
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=epochs,
        eta_min=learning_rate * 0.1
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if use_amp and AMP_AVAILABLE else None
    
    # è·å–ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    effective_batch_size = batch_size * gradient_accumulation_steps
    dataloader, dataset = get_optimized_cifar10_dataloader(
        batch_size=batch_size, 
        num_workers=8  # å¢åŠ workeræ•°é‡
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('samples', exist_ok=True)
    os.makedirs('optimized_training', exist_ok=True)
    
    # è®­ç»ƒå†å²è®°å½•
    losses = []
    best_loss = float('inf')
    
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   - è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   - å®é™…æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    print(f"   - åˆå§‹å­¦ä¹ ç‡: {learning_rate}")
    print(f"   - æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   - é¢„è®¡æ¯epochæ‰¹æ¬¡æ•°: {len(dataloader)}")
    print("=" * 60)
    
    # å¼€å§‹è®­ç»ƒ
    training_start_time = time.time()
    
    for epoch in range(epochs):
        model.unet.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        # é‡ç½®æ¢¯åº¦ç´¯ç§¯
        optimizer.zero_grad()
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            if use_amp and scaler is not None:
                with autocast():
                    loss = model.train_step(data) / gradient_accumulation_steps
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                # æ ‡å‡†è®­ç»ƒ
                loss = model.train_step(data) / gradient_accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
            
            epoch_losses.append(loss.item() * gradient_accumulation_steps)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = scheduler.get_last_lr()[0]
            pbar.set_postfix({
                'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                'lr': f'{current_lr:.2e}'
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'ğŸ“Š Epoch {epoch+1}/{epochs}:')
        print(f'   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’')
        print(f'   ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'   ğŸ“ˆ å­¦ä¹ ç‡: {current_lr:.2e}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            if efficient_checkpointing:
                best_path = 'checkpoints/ddpm_best.pt'
                model.save_model(best_path)
                print(f'   ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜')
        
        # å¿«é€Ÿé‡‡æ ·ç›‘æ§
        if (epoch + 1) % fast_sampling_interval == 0:
            model.unet.eval()
            with torch.no_grad():
                # æ›´å¿«çš„é‡‡æ ·ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ï¼‰
                quick_samples = model.sample(batch_size=4)
                save_optimized_samples(
                    quick_samples, 
                    f'optimized_training/quick_epoch_{epoch+1}.png',
                    epoch=epoch+1,
                    loss=avg_loss
                )
            print(f'   ğŸ“¸ å¿«é€Ÿé‡‡æ ·å·²ä¿å­˜')
        
        # å®Œæ•´æ£€æŸ¥ç‚¹ä¿å­˜
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoints/ddpm_epoch_{epoch+1}.pt'
            model.save_model(checkpoint_path)
            
            # ç”Ÿæˆå®Œæ•´æ ·æœ¬
            model.unet.eval()
            with torch.no_grad():
                samples = model.sample(batch_size=16)
                save_optimized_samples(
                    samples, 
                    f'samples/epoch_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss
                )
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            plot_optimized_losses(losses, f'samples/loss_curve_epoch_{epoch+1}.png')
            print(f'   ğŸ’¾ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜')
        
        print("-" * 50)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - training_start_time
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = 'checkpoints/ddpm_final_optimized.pt'
    model.save_model(final_path)
    
    print("ğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸ“‰ æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    
    return model, losses

def save_optimized_samples(samples, path, epoch=None, loss=None):
    """ä¼˜åŒ–çš„æ ·æœ¬ä¿å­˜å‡½æ•°"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        grid = torchvision.utils.make_grid(samples, nrow=4, padding=2)
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        plt.figure(figsize=(10, 8))
        plt.imshow(grid_np)
        plt.axis('off')
        
        if epoch is not None and loss is not None:
            plt.title(f'Epoch {epoch} - Loss: {loss:.4f}', fontsize=14, fontweight='bold')
        
        plt.savefig(path, bbox_inches='tight', dpi=100)  # é™ä½DPIåŠ é€Ÿä¿å­˜
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_optimized_losses(losses, path):
    """ä¼˜åŒ–çš„æŸå¤±ç»˜åˆ¶å‡½æ•°"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(10, 6))
        plt.plot(losses, linewidth=2)
        plt.title('Training Loss (Optimized)', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æœ€å°å€¼æ ‡è®°
        min_loss_idx = np.argmin(losses)
        min_loss = losses[min_loss_idx]
        plt.scatter([min_loss_idx], [min_loss], color='red', s=100, zorder=5)
        plt.annotate(f'Best: {min_loss:.4f}', 
                    xy=(float(min_loss_idx), float(min_loss)),
                    xytext=(10, 10), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))
        
        plt.savefig(path, bbox_inches='tight', dpi=100)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    use_amp = '--no-amp' not in sys.argv
    compile_model = '--no-compile' not in sys.argv
    
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    print(f"æ··åˆç²¾åº¦: {'å¼€å¯' if use_amp else 'å…³é—­'}")
    print(f"æ¨¡å‹ç¼–è¯‘: {'å¼€å¯' if compile_model else 'å…³é—­'}")
    
    try:
        # é’ˆå¯¹ä¸åŒGPUä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if '3090' in gpu_name or '4090' in gpu_name:
                batch_size = 32
                gradient_accumulation = 1
            elif '3060' in gpu_name:
                batch_size = 16
                gradient_accumulation = 2  # é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§batch
            else:
                batch_size = 16
                gradient_accumulation = 1
        else:
            batch_size = 8
            gradient_accumulation = 1
        
        model, losses = train_ddpm_optimized(
            epochs=50,
            batch_size=batch_size,
            learning_rate=2e-4,  # ç¨å¾®æé«˜å­¦ä¹ ç‡
            save_interval=10,
            use_amp=use_amp,
            gradient_accumulation_steps=gradient_accumulation,
            compile_model=compile_model,
            fast_sampling_interval=5
        )
        
        print("ğŸ‰ ä¼˜åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 