import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
import time

# æ£€æŸ¥æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False

def get_stl10_optimized_dataloader(batch_size=8, num_workers=None):
    """è·å–ä¼˜åŒ–çš„STL-10æ•°æ®åŠ è½½å™¨"""
    
    if num_workers is None:
        num_workers = min(8, os.cpu_count() or 4)
    
    print(f"ğŸ“Š STL-10ä¼˜åŒ–æ•°æ®é›†é…ç½®:")
    print(f"   - åŸç”Ÿåˆ†è¾¨ç‡: 96x96")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - å·¥ä½œè¿›ç¨‹: {num_workers}")
    
    # STL-10ç‰¹åŒ–çš„æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        # STL-10å·²ç»æ˜¯96x96ï¼Œæ— éœ€resize
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=5),  # è½»å¾®æ—‹è½¬
        # è½»å¾®çš„é¢œè‰²å¢å¼º - STL-10å›¾åƒè´¨é‡è¾ƒé«˜ï¼Œå¢å¼ºè¦æ¸©å’Œ
        transforms.ColorJitter(
            brightness=0.05,  # å¾ˆè½»å¾®çš„äº®åº¦è°ƒæ•´
            contrast=0.05,    # å¾ˆè½»å¾®çš„å¯¹æ¯”åº¦è°ƒæ•´
            saturation=0.05,  # å¾ˆè½»å¾®çš„é¥±å’Œåº¦è°ƒæ•´
            hue=0.02         # å¾ˆè½»å¾®çš„è‰²è°ƒè°ƒæ•´
        ),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1,1]
    ])
    
    # åˆ›å»ºSTL-10æ•°æ®é›†
    dataset = torchvision.datasets.STL10(
        root='./data',
        split='train',  # ä½¿ç”¨è®­ç»ƒé›†
        download=True,
        transform=transform
    )
    
    print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=3,  # å¢åŠ é¢„å–å› å­
        drop_last=True
    )
    
    return dataloader, dataset

def train_stl10_optimized_ddpm(
    epochs=100,  # å¢åŠ è®­ç»ƒè½®æ•°
    batch_size=None,
    learning_rate=1e-4,
    save_interval=10,
    use_amp=True,
    compile_model=True,
    use_ema=True,  # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
    warmup_epochs=5  # å­¦ä¹ ç‡çƒ­èº«
):
    """ä¼˜åŒ–çš„STL-10 DDPMè®­ç»ƒ"""
    
    print("ğŸ¯ STL-10ä¼˜åŒ–DDPMè®­ç»ƒ")
    print("=" * 60)
    
    # è·å–GPUä¿¡æ¯å¹¶è°ƒæ•´æ‰¹æ¬¡å¤§å°
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        if '3060' in gpu_name:
            gpu_memory = 12
            recommended_batch = 6  # ä¿å®ˆä¸€äº›
        elif '3090' in gpu_name or '4090' in gpu_name:
            gpu_memory = 24
            recommended_batch = 12
        elif '3080' in gpu_name:
            gpu_memory = 10
            recommended_batch = 6
        else:
            gpu_memory = 8
            recommended_batch = 4
        print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({gpu_memory}GB)")
    else:
        gpu_memory = 0
        recommended_batch = 2
        print("ğŸ–¥ï¸  è®¾å¤‡: CPU")
    
    if batch_size is None:
        batch_size = recommended_batch
    
    # è·å–æ•°æ®åŠ è½½å™¨
    dataloader, dataset = get_stl10_optimized_dataloader(batch_size=batch_size)
    
    # STL-10ä¸“ç”¨çš„å¼ºåŒ–æ¨¡å‹é…ç½®
    model_config = {
        'dim': 256,  # æ˜¾è‘—å¢åŠ åŸºç¡€ç»´åº¦
        'dim_mults': (1, 1, 2, 2, 4, 4, 8),  # æ›´æ·±çš„å±‚çº§ç»“æ„
        # 'flash_attn': True,  # å¦‚æœå¯ç”¨çš„è¯
    }
    
    print(f"ğŸ—ï¸  STL-10ä¼˜åŒ–æ¨¡å‹é…ç½®:")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ¨¡å‹ç»´åº¦: {model_config['dim']}")
    print(f"   - å±‚çº§å€æ•°: {model_config['dim_mults']}")
    print(f"   - æ€»è½®æ•°: {epochs}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DDPMModel(
        image_size=96,
        channels=3,
        timesteps=1000,  # ä¿æŒ1000æ­¥
        **model_config
    )
    
    total_params = sum(p.numel() for p in model.unet.parameters())
    print(f"   - æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # æ¨¡å‹ç¼–è¯‘
    if compile_model and hasattr(torch, 'compile'):
        try:
            compiled_unet = torch.compile(model.unet, mode='reduce-overhead')
            model.unet = compiled_unet  # type: ignore
            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ (reduce-overheadæ¨¡å¼)")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer = optim.AdamW(
        model.unet.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # æ›´å¤æ‚çš„å­¦ä¹ ç‡è°ƒåº¦
    warmup_scheduler = optim.lr_scheduler.LinearLR(
        optimizer, 
        start_factor=0.1, 
        end_factor=1.0, 
        total_iters=warmup_epochs
    )
    
    main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=epochs - warmup_epochs, 
        eta_min=learning_rate * 0.01  # æ›´ä½çš„æœ€å°å­¦ä¹ ç‡
    )
    
    # æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰
    if use_ema:
        try:
            from torch_ema import ExponentialMovingAverage
            ema = ExponentialMovingAverage(model.unet.parameters(), decay=0.995)
            print("âœ… EMAåˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            print("âš ï¸ torch_emaæœªå®‰è£…ï¼Œè·³è¿‡EMA")
            print("ğŸ’¡ å¯ä»¥é€šè¿‡ pip install torch-ema å®‰è£…")
            use_ema = False
            ema = None
    else:
        ema = None
    
    # æ··åˆç²¾åº¦
    scaler = GradScaler() if use_amp and AMP_AVAILABLE else None
    if scaler:
        print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå¯ç”¨")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    exp_name = "stl10_optimized_96x96"
    os.makedirs(f'checkpoints/{exp_name}', exist_ok=True)
    os.makedirs(f'samples/{exp_name}', exist_ok=True)
    os.makedirs(f'training_progress/{exp_name}', exist_ok=True)
    
    print(f"ğŸ“ å®éªŒåç§°: {exp_name}")
    print("=" * 70)
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    best_loss = float('inf')
    training_start_time = time.time()
    
    for epoch in range(epochs):
        model.unet.train()
        epoch_losses = []
        epoch_start_time = time.time()
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if epoch < warmup_epochs:
            warmup_scheduler.step()
        else:
            main_scheduler.step()
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            optimizer.zero_grad()
            
            if use_amp and scaler is not None:
                with autocast():
                    loss = model.train_step(data)
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.unet.parameters(), max_norm=1.0)
                
                scaler.step(optimizer)
                scaler.update()
            else:
                loss = model.train_step(data)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.unet.parameters(), max_norm=1.0)
                
                optimizer.step()
            
            # æ›´æ–°EMA
            if use_ema and ema is not None:
                ema.update()
            
            epoch_losses.append(loss.item())
            
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{current_lr:.2e}',
                'ema': 'ON' if use_ema else 'OFF'
            })
        
        # Epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        print(f'ğŸ“Š Epoch {epoch+1}/{epochs}:')
        print(f'   â±ï¸  ç”¨æ—¶: {epoch_time:.2f}ç§’')
        print(f'   ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}')
        print(f'   ğŸ“ˆ å­¦ä¹ ç‡: {current_lr:.2e}')
        
        # æŸå¤±è´¨é‡è¯„ä¼°
        if avg_loss > 0.5:
            quality = "ğŸ”´ çº¯å™ªå£°é˜¶æ®µ"
        elif avg_loss > 0.3:
            quality = "ğŸŸ¡ å½¢çŠ¶å½¢æˆé˜¶æ®µ"
        elif avg_loss > 0.15:
            quality = "ğŸŸ¢ å¯è¾¨è¯†ç‰©ä½“é˜¶æ®µ"
        elif avg_loss > 0.08:
            quality = "ğŸ”µ æ¸…æ™°å›¾åƒé˜¶æ®µ"
        else:
            quality = "ğŸŸ£ é«˜è´¨é‡é˜¶æ®µ"
        
        print(f'   ğŸ¯ è´¨é‡è¯„ä¼°: {quality}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = f'checkpoints/{exp_name}/ddpm_best.pt'
            
            # ä½¿ç”¨EMAæƒé‡ä¿å­˜æœ€ä½³æ¨¡å‹
            if use_ema and ema is not None:
                with ema.average_parameters():
                    model.save_model(best_path)
            else:
                model.save_model(best_path)
            print(f'   ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_loss:.4f})')
        
        # å®šæœŸé‡‡æ ·
        if (epoch + 1) % 5 == 0:
            model.unet.eval()
            with torch.no_grad():
                # ä½¿ç”¨EMAæƒé‡è¿›è¡Œé‡‡æ ·
                if use_ema and ema is not None:
                    with ema.average_parameters():
                        quick_samples = model.sample(batch_size=9)
                else:
                    quick_samples = model.sample(batch_size=9)
                
                save_stl10_samples(
                    quick_samples,
                    f'training_progress/{exp_name}/quick_epoch_{epoch+1}.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    quality=quality
                )
            print(f'   ğŸ“¸ å¿«é€Ÿé‡‡æ ·å·²ä¿å­˜')
        
        # æ£€æŸ¥ç‚¹ä¿å­˜
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoints/{exp_name}/ddpm_epoch_{epoch+1}.pt'
            
            # ä¿å­˜å¸¸è§„æƒé‡
            model.save_model(checkpoint_path)
            
            # å¦‚æœä½¿ç”¨EMAï¼Œä¹Ÿä¿å­˜EMAæƒé‡
            if use_ema and ema is not None:
                ema_path = f'checkpoints/{exp_name}/ddpm_ema_epoch_{epoch+1}.pt'
                with ema.average_parameters():
                    model.save_model(ema_path)
            
            # ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬
            model.unet.eval()
            with torch.no_grad():
                if use_ema and ema is not None:
                    with ema.average_parameters():
                        samples = model.sample(batch_size=16)
                else:
                    samples = model.sample(batch_size=16)
                
                save_stl10_samples(
                    samples,
                    f'samples/{exp_name}/epoch_{epoch+1}_samples.png',
                    epoch=epoch+1,
                    loss=avg_loss,
                    quality=quality
                )
            
            plot_stl10_losses(losses, f'samples/{exp_name}/loss_curve_epoch_{epoch+1}.png', exp_name)
            print(f'   ğŸ’¾ å®Œæ•´æ£€æŸ¥ç‚¹å·²ä¿å­˜')
        
        print("-" * 60)
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - training_start_time
    final_path = f'checkpoints/{exp_name}/ddpm_final.pt'
    
    # ä½¿ç”¨EMAæƒé‡ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if use_ema and ema is not None:
        with ema.average_parameters():
            model.save_model(final_path)
        
        # ä¹Ÿä¿å­˜éEMAç‰ˆæœ¬
        regular_path = f'checkpoints/{exp_name}/ddpm_final_regular.pt'
        model.save_model(regular_path)
    else:
        model.save_model(final_path)
    
    print("ğŸ‰ STL-10ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸ“‰ æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    
    return model, losses

def save_stl10_samples(samples, path, epoch=None, loss=None, quality=""):
    """ä¿å­˜STL-10æ ·æœ¬"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        samples = (samples + 1) / 2
        samples = torch.clamp(samples, 0, 1)
        
        nrow = 3 if len(samples) <= 9 else 4
        grid = torchvision.utils.make_grid(samples, nrow=nrow, padding=2)
        grid_np = grid.permute(1, 2, 0).cpu().numpy()
        
        plt.figure(figsize=(12, 12))
        plt.imshow(grid_np)
        plt.axis('off')
        
        title = f'STL-10 Epoch {epoch} - Loss: {loss:.4f} (96x96)'
        if quality:
            title += f'\n{quality}'
        
        if epoch is not None and loss is not None:
            plt.title(title, fontsize=14, fontweight='bold', pad=20)
        
        plt.savefig(path, bbox_inches='tight', dpi=200)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜STL-10æ ·æœ¬æ—¶å‡ºé”™: {e}")

def plot_stl10_losses(losses, path, exp_name):
    """ç»˜åˆ¶STL-10è®­ç»ƒæŸå¤±"""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        plt.figure(figsize=(12, 8))
        plt.plot(losses, linewidth=2, color='blue')
        plt.title(f'STL-10 Training Loss - {exp_name}', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ è´¨é‡é˜¶æ®µæ ‡è®°
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='çº¯å™ªå£°é˜¶æ®µ')
        plt.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='å½¢çŠ¶å½¢æˆé˜¶æ®µ')
        plt.axhline(y=0.15, color='green', linestyle='--', alpha=0.7, label='å¯è¾¨è¯†ç‰©ä½“é˜¶æ®µ')
        plt.axhline(y=0.08, color='blue', linestyle='--', alpha=0.7, label='æ¸…æ™°å›¾åƒé˜¶æ®µ')
        
        if losses:
            min_loss_idx = np.argmin(losses)
            min_loss = losses[min_loss_idx]
            plt.scatter([min_loss_idx], [min_loss], color='red', s=100, zorder=5)
            plt.annotate(f'Best: {min_loss:.4f}', 
                        xy=(float(min_loss_idx), float(min_loss)),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))
        
        plt.legend()
        plt.tight_layout()
        plt.savefig(path, bbox_inches='tight', dpi=150)
        plt.close()
        
    except Exception as e:
        print(f"ä¿å­˜STL-10æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    print("ğŸ¯ STL-10ä¼˜åŒ–DDPMè®­ç»ƒ")
    print("ä¸“é—¨é’ˆå¯¹STL-10 (96x96) çš„å¢å¼ºé…ç½®")
    print("=" * 70)
    
    # è§£æå‚æ•°
    epochs = int(sys.argv[1]) if len(sys.argv) > 1 else 100
    batch_size = int(sys.argv[2]) if len(sys.argv) > 2 else None
    
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size or 'è‡ªåŠ¨'}")
    print("=" * 70)
    
    try:
        model, losses = train_stl10_optimized_ddpm(
            epochs=epochs,
            batch_size=batch_size,
            use_amp='--no-amp' not in sys.argv,
            compile_model='--no-compile' not in sys.argv,
            use_ema='--no-ema' not in sys.argv
        )
        
        print("ğŸ‰ STL-10ä¼˜åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®è¿è¡Œæ¨ç†æŸ¥çœ‹ç”Ÿæˆè´¨é‡ï¼š")
        print("   python inference_high_res.py --model checkpoints/stl10_optimized_96x96/ddpm_best.pt")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 