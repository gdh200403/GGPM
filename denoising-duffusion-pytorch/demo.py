import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import numpy as np
import os
from ddpm_model import DDPMModel
from tqdm import tqdm
import time

# åˆ›å»ºdemoç»“æœç›®å½•
DEMO_OUTPUT_DIR = 'demo_results'
os.makedirs(DEMO_OUTPUT_DIR, exist_ok=True)

def quick_data_test():
    """å¿«é€Ÿæµ‹è¯•æ•°æ®åŠ è½½"""
    print("=== æµ‹è¯•æ•°æ®åŠ è½½ ===")
    
    transform = transforms.Compose([
        transforms.Resize(32),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    dataset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )
    
    # ä½¿ç”¨å°éƒ¨åˆ†æ•°æ®è¿›è¡Œæµ‹è¯•
    subset = Subset(dataset, range(100))  # åªä½¿ç”¨100ä¸ªæ ·æœ¬
    dataloader = DataLoader(subset, batch_size=8, shuffle=True)
    
    print(f"æ•°æ®åŠ è½½æˆåŠŸï¼")
    print(f"- æ•°æ®é›†å¤§å°: {len(subset)}")
    print(f"- æ‰¹æ¬¡å¤§å°: 8")
    print(f"- å›¾åƒå°ºå¯¸: {dataset[0][0].shape}")
    
    # å¯è§†åŒ–ä¸€äº›æ ·æœ¬
    data_iter = iter(dataloader)
    images, labels = next(data_iter)
    
    # ä¿å­˜æ ·æœ¬
    images_display = (images + 1) / 2  # è½¬æ¢åˆ°[0,1]
    grid = torchvision.utils.make_grid(images_display, nrow=4)
    
    plt.figure(figsize=(8, 6))
    plt.imshow(grid.permute(1, 2, 0))
    plt.axis('off')
    plt.title('CIFAR-10 Sample Batch')
    sample_path = os.path.join(DEMO_OUTPUT_DIR, 'demo_data_samples.png')
    plt.savefig(sample_path, bbox_inches='tight', dpi=150)
    plt.close()
    
    print(f"æ ·æœ¬å›¾åƒå·²ä¿å­˜åˆ°: {sample_path}")
    return dataloader

def quick_model_test():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•æ¨¡å‹åˆå§‹åŒ– ===")
    
    # åˆ›å»ºå°æ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    model = DDPMModel(
        image_size=32,
        channels=3,
        dim=32,  # å‡å°æ¨¡å‹å°ºå¯¸
        dim_mults=(1, 2, 4),  # å‡å°‘å±‚æ•°
        timesteps=100  # å‡å°‘æ—¶é—´æ­¥æ•°
    )
    
    print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 4
    test_input = torch.randn(batch_size, 3, 32, 32).to(model.device)
    
    with torch.no_grad():
        loss = model.train_step(test_input)
        print(f"å‰å‘ä¼ æ’­æµ‹è¯• - æŸå¤±å€¼: {loss.item():.4f}")
        
        # æµ‹è¯•é‡‡æ ·
        samples = model.sample(batch_size=4)
        print(f"é‡‡æ ·æµ‹è¯• - ç”Ÿæˆæ ·æœ¬å½¢çŠ¶: {samples.shape}")
    
    return model

def quick_training_test(model, dataloader, epochs=3):
    """å¿«é€Ÿè®­ç»ƒæµ‹è¯•"""
    print(f"\n=== å¿«é€Ÿè®­ç»ƒæµ‹è¯• ({epochs} epochs) ===")
    
    optimizer = torch.optim.Adam(model.unet.parameters(), lr=1e-3)
    losses = []
    
    model.unet.train()
    
    for epoch in range(epochs):
        epoch_losses = []
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')
        for batch_idx, (data, _) in enumerate(pbar):
            optimizer.zero_grad()
            
            loss = model.train_step(data)
            loss.backward()
            optimizer.step()
            
            epoch_losses.append(loss.item())
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        print(f'Epoch {epoch+1}, å¹³å‡æŸå¤±: {avg_loss:.4f}')
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(8, 5))
    plt.plot(losses, marker='o')
    plt.title('Quick Training - Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    sample_path = os.path.join(DEMO_OUTPUT_DIR, 'demo_training_loss.png')
    plt.savefig(sample_path, bbox_inches='tight', dpi=150)
    plt.close()
    
    print(f"è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {sample_path}")
    
    # ä¿å­˜å¿«é€Ÿè®­ç»ƒçš„æ¨¡å‹
    model_path = os.path.join(DEMO_OUTPUT_DIR, 'demo_model.pt')
    model.save_model(model_path)
    
    return model, losses

def quick_inference_test(model):
    """å¿«é€Ÿæ¨ç†æµ‹è¯•"""
    print("\n=== å¿«é€Ÿæ¨ç†æµ‹è¯• ===")
    
    model.unet.eval()
    
    with torch.no_grad():
        # ç”Ÿæˆå°‘é‡æ ·æœ¬
        print("ç”Ÿæˆæ ·æœ¬...")
        samples = model.sample(batch_size=8)
        
        # ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬
        samples_display = (samples + 1) / 2
        samples_display = torch.clamp(samples_display, 0, 1)
        
        grid = torchvision.utils.make_grid(samples_display, nrow=4, padding=2)
        
        plt.figure(figsize=(8, 6))
        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())
        plt.axis('off')
        plt.title('Generated Samples (Quick Demo)')
        sample_path = os.path.join(DEMO_OUTPUT_DIR, 'demo_generated_samples.png')
        plt.savefig(sample_path, bbox_inches='tight', dpi=150)
        plt.close()
        
        print(f"ç”Ÿæˆæ ·æœ¬å·²ä¿å­˜åˆ°: {sample_path}")
    
    return samples

def advanced_correctness_test():
    """é«˜çº§æ­£ç¡®æ€§éªŒè¯æµ‹è¯•"""
    print("\n=== é«˜çº§æ­£ç¡®æ€§éªŒè¯ ===")
    
    model = DDPMModel(
        image_size=32,
        channels=3,
        dim=32,
        dim_mults=(1, 2, 4),
        timesteps=100
    )
    
    # 1. æµ‹è¯•æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦æ­£ç¡®æ€§
    print("1. éªŒè¯æ‰©æ•£è¿‡ç¨‹æ•°å­¦æ€§è´¨...")
    batch_size = 4
    test_images = torch.randn(batch_size, 3, 32, 32).to(model.device)
    
    with torch.no_grad():
        # æµ‹è¯•å™ªå£°è°ƒåº¦çš„å•è°ƒæ€§
        timesteps = [10, 20, 50, 80]
        noise_levels = []
        
        for t in timesteps:
            t_tensor = torch.full((batch_size,), t, device=model.device, dtype=torch.long)
            # è¿™é‡Œéœ€è¦è®¿é—®æ‰©æ•£è¿‡ç¨‹çš„å†…éƒ¨çŠ¶æ€æ¥éªŒè¯
            # ç®€åŒ–ç‰ˆæœ¬çš„å™ªå£°éªŒè¯
            noisy = model.diffusion.q_sample(test_images, t_tensor)
            noise_level = torch.mean((noisy - test_images) ** 2).item()
            noise_levels.append(noise_level)
        
        # éªŒè¯å™ªå£°æ°´å¹³éšæ—¶é—´æ­¥é€’å¢
        is_monotonic = all(noise_levels[i] <= noise_levels[i+1] for i in range(len(noise_levels)-1))
        print(f"   å™ªå£°è°ƒåº¦å•è°ƒæ€§: {'âœ… é€šè¿‡' if is_monotonic else 'âŒ å¤±è´¥'}")
        print(f"   å™ªå£°æ°´å¹³: {[f'{x:.3f}' for x in noise_levels]}")
    
    # 2. æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§
    print("2. éªŒè¯æ¢¯åº¦ç¨³å®šæ€§...")
    model.unet.train()
    test_batch = torch.randn(2, 3, 32, 32).to(model.device)
    
    # è®¡ç®—æ¢¯åº¦
    loss = model.train_step(test_batch)
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
    total_grad_norm = 0
    for param in model.unet.parameters():
        if param.grad is not None:
            total_grad_norm += param.grad.data.norm(2).item() ** 2
    total_grad_norm = total_grad_norm ** 0.5
    
    print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.4f}")
    print(f"   æ¢¯åº¦ç¨³å®šæ€§: {'âœ… é€šè¿‡' if total_grad_norm < 100 else 'âŒ å¯èƒ½ä¸ç¨³å®š'}")
    
    # 3. æµ‹è¯•é‡‡æ ·ä¸€è‡´æ€§
    print("3. éªŒè¯é‡‡æ ·ä¸€è‡´æ€§...")
    model.unet.eval()
    
    # å›ºå®šéšæœºç§å­ï¼Œå¤šæ¬¡é‡‡æ ·åº”è¯¥å¾—åˆ°ç›¸åŒç»“æœ
    torch.manual_seed(42)
    sample1 = model.sample(batch_size=2)
    
    torch.manual_seed(42)  
    sample2 = model.sample(batch_size=2)
    
    consistency_error = torch.mean((sample1 - sample2) ** 2).item()
    print(f"   é‡‡æ ·ä¸€è‡´æ€§è¯¯å·®: {consistency_error:.6f}")
    print(f"   ä¸€è‡´æ€§æµ‹è¯•: {'âœ… é€šè¿‡' if consistency_error < 1e-5 else 'âŒ å¤±è´¥'}")
    
    # 4. æµ‹è¯•æ¨¡å‹ä¿å­˜/åŠ è½½å®Œæ•´æ€§
    print("4. éªŒè¯æ¨¡å‹ä¿å­˜/åŠ è½½å®Œæ•´æ€§...")
    
    # ä¿å­˜åŸå§‹æ¨¡å‹çš„ä¸€ä¸ªæ ·æœ¬
    torch.manual_seed(123)
    original_sample = model.sample(batch_size=1)
    
    # ä¿å­˜å¹¶é‡æ–°åŠ è½½æ¨¡å‹
    temp_path = os.path.join(DEMO_OUTPUT_DIR, 'temp_test_model.pt')
    model.save_model(temp_path)
    
    new_model = DDPMModel(
        image_size=32,
        channels=3, 
        dim=32,
        dim_mults=(1, 2, 4),
        timesteps=100
    )
    new_model.load_model(temp_path)
    new_model.unet.eval()
    
    # ä½¿ç”¨ç›¸åŒéšæœºç§å­ç”Ÿæˆæ ·æœ¬
    torch.manual_seed(123)
    loaded_sample = new_model.sample(batch_size=1)
    
    load_error = torch.mean((original_sample - loaded_sample) ** 2).item()
    print(f"   åŠ è½½è¯¯å·®: {load_error:.6f}")
    print(f"   ä¿å­˜/åŠ è½½æµ‹è¯•: {'âœ… é€šè¿‡' if load_error < 1e-5 else 'âŒ å¤±è´¥'}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(temp_path):
        os.remove(temp_path)
    
    return True

def full_demo():
    """å®Œæ•´çš„demoæµ‹è¯•æµç¨‹"""
    print("ğŸš€ å¼€å§‹DDPMæ¨¡å‹Demoæµ‹è¯•")
    print("=" * 50)
    
    start_time = time.time()
    
    try:
        # 1. æµ‹è¯•æ•°æ®åŠ è½½
        dataloader = quick_data_test()
        
        # 2. æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–
        model = quick_model_test()
        
        # 3. å¿«é€Ÿè®­ç»ƒæµ‹è¯•
        model, losses = quick_training_test(model, dataloader, epochs=5)
        
        # 4. å¿«é€Ÿæ¨ç†æµ‹è¯•
        samples = quick_inference_test(model)
        
        # 5. é«˜çº§æ­£ç¡®æ€§éªŒè¯
        advanced_correctness_test()
        
        # 6. æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
        print("\n=== æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ===")
        
        # é‡æ–°åŠ è½½æ¨¡å‹
        model_reloaded = DDPMModel(
            image_size=32,
            channels=3,
            dim=32,
            dim_mults=(1, 2, 4),
            timesteps=100
        )
        model_reloaded.load_model(os.path.join(DEMO_OUTPUT_DIR, 'demo_model.pt'))
        
        # æµ‹è¯•é‡æ–°åŠ è½½çš„æ¨¡å‹
        with torch.no_grad():
            test_samples = model_reloaded.sample(batch_size=4)
            print(f"é‡æ–°åŠ è½½æ¨¡å‹é‡‡æ ·æµ‹è¯•æˆåŠŸ - å½¢çŠ¶: {test_samples.shape}")
        
        end_time = time.time()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Demoæµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
        print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶éƒ½ä¿å­˜åœ¨ '{DEMO_OUTPUT_DIR}' ç›®å½•ä¸­:")
        print(f"- {os.path.join(DEMO_OUTPUT_DIR, 'demo_data_samples.png')} (æ•°æ®æ ·æœ¬)")
        print(f"- {os.path.join(DEMO_OUTPUT_DIR, 'demo_training_loss.png')} (è®­ç»ƒæŸå¤±)")
        print(f"- {os.path.join(DEMO_OUTPUT_DIR, 'demo_generated_samples.png')} (ç”Ÿæˆæ ·æœ¬)")
        print(f"- {os.path.join(DEMO_OUTPUT_DIR, 'demo_model.pt')} (æ¨¡å‹æƒé‡)")
        print(f"- {os.path.join(DEMO_OUTPUT_DIR, 'demo_benchmark.png')} (æ€§èƒ½åŸºå‡†)")
        
        print("\nâœ… æ‰€æœ‰ç»„ä»¶å·¥ä½œæ­£å¸¸ï¼Œæ ¸å¿ƒä»£ç éªŒè¯é€šè¿‡ï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Demoæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def benchmark_test():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"æµ‹è¯•è®¾å¤‡: {device}")
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½
    batch_sizes = [1, 4, 8, 16] if device.type == 'cuda' else [1, 2, 4]
    
    model = DDPMModel(
        image_size=32,
        channels=3,
        dim=32,
        dim_mults=(1, 2, 4),
        timesteps=100
    )
    
    model.unet.eval()
    
    results = []
    
    for batch_size in batch_sizes:
        print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # é¢„çƒ­
        with torch.no_grad():
            dummy_input = torch.randn(batch_size, 3, 32, 32).to(device)
            _ = model.train_step(dummy_input)
        
        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        times = []
        for _ in range(5):
            start = time.time()
            with torch.no_grad():
                _ = model.sample(batch_size=batch_size)
            end = time.time()
            times.append(end - start)
        
        avg_time = np.mean(times)
        results.append((batch_size, avg_time))
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}ç§’")
        print(f"æ¯æ ·æœ¬æ—¶é—´: {avg_time/batch_size:.3f}ç§’")
    
    # ç»˜åˆ¶æ€§èƒ½å›¾è¡¨
    batch_sizes, times = zip(*results)
    
    plt.figure(figsize=(10, 6))
    plt.subplot(1, 2, 1)
    plt.plot(batch_sizes, times, 'o-')
    plt.title('Batch Size vs Total Time')
    plt.xlabel('Batch Size')
    plt.ylabel('Time (seconds)')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    per_sample_times = [t/b for b, t in results]
    plt.plot(batch_sizes, per_sample_times, 'o-')
    plt.title('Batch Size vs Per-Sample Time')
    plt.xlabel('Batch Size')
    plt.ylabel('Time per Sample (seconds)')
    plt.grid(True)
    
    plt.tight_layout()
    sample_path = os.path.join(DEMO_OUTPUT_DIR, 'demo_benchmark.png')
    plt.savefig(sample_path, bbox_inches='tight', dpi=150)
    plt.close()
    
    print(f"æ€§èƒ½åŸºå‡†å›¾è¡¨å·²ä¿å­˜åˆ°: {sample_path}")

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´demo
    success = full_demo()
    
    if success:
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        benchmark_test()
        
        print("\nğŸ”¥ æ¥ä¸‹æ¥æ‚¨å¯ä»¥:")
        print("1. è¿è¡Œ 'python train.py' è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        print("2. è¿è¡Œ 'python inference.py' è¿›è¡Œæ¨ç†")
        print("3. ä¿®æ”¹è¶…å‚æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½")
    else:
        print("\nğŸ”§ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜") 